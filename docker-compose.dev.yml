services:
  node-red:
    build:
      context: .
      dockerfile: Dockerfile
    image: node-red-condition-monitoring:dev
    container_name: node-red-condition-monitoring-dev
    user: "1000:1000"
    ports:
      - "1880:1880"
    volumes:
      # Node-RED Daten (Flows, Settings)
      - ./node-red-data:/data
      # Modul-Nodes für Live-Entwicklung mounten (überschreibt das COPY im Dockerfile)
      - ./nodes:/data/node_modules/node-red-contrib-condition-monitoring/nodes:ro
      - ./package.json:/data/node_modules/node-red-contrib-condition-monitoring/package.json:ro
      # ML Models Verzeichnis
      - ./models:/data/models:ro
    environment:
      - NODE_OPTIONS=--max_old_space_size=4096
      - NODE_ENV=development
      - TZ=Europe/Berlin
      # MLflow Server URL für Integration (interner Container-Name)
      - MLFLOW_TRACKING_URI=http://mlflow:5000
      # MAX Engine URL für High-Performance Inference
      - MAX_ENGINE_URL=http://max-engine:8765
      # Von Node-RED aus erreichbar als http://mlflow:5000
    restart: unless-stopped
    depends_on:
      - mlflow
      - max-engine
    networks:
      - node-red-network

  # MLflow Server für Model Registry und Experiment Tracking
  # UI: http://localhost:5050
  # API: http://localhost:5050/api/2.0/mlflow/
  mlflow:
    image: ghcr.io/mlflow/mlflow:v2.17.2
    container_name: mlflow-server
    ports:
      - "5050:5000"
    volumes:
      - mlflow-data:/mlflow
      - ./models:/mlflow/models:ro
    environment:
      - MLFLOW_BACKEND_STORE_URI=sqlite:///mlflow/mlflow.db
      - MLFLOW_DEFAULT_ARTIFACT_ROOT=/mlflow/artifacts
    command: >
      mlflow server
      --host 0.0.0.0
      --port 5000
      --backend-store-uri sqlite:///mlflow/mlflow.db
      --default-artifact-root /mlflow/artifacts
    restart: unless-stopped
    networks:
      - node-red-network

  # MAX Engine Server for high-performance ONNX inference
  # API: http://localhost:8765 (internal: http://max-engine:8765)
  # Uses Modular MAX Engine for optimized ML inference
  max-engine:
    build:
      context: .
      dockerfile: Dockerfile.max
    image: max-engine-inference:dev
    container_name: max-engine-inference
    ports:
      - "8765:8765"
    volumes:
      - ./models:/models:ro
      - ./nodes/max_bridge.py:/app/max_bridge.py:ro
      - max-cache:/root/.cache/max
    environment:
      - MAX_BRIDGE_PORT=8765
      - MAX_BRIDGE_HOST=0.0.0.0
      - PYTHONUNBUFFERED=1
    restart: unless-stopped
    networks:
      - node-red-network
    # Note: Add --gpus=1 if you have NVIDIA GPU
    # deploy:
    #   resources:
    #     reservations:
    #       devices:
    #         - driver: nvidia
    #           count: 1
    #           capabilities: [gpu]

volumes:
  mlflow-data:
  max-cache:

networks:
  node-red-network:
    driver: bridge

