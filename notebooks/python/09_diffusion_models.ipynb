{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Diffusion Models for Predictive Maintenance\n",
    "\n",
    "This notebook implements **Diffusion Models** for condition monitoring:\n",
    "\n",
    "1. **Denoising Diffusion for Anomaly Detection** - Reconstruction-based detection\n",
    "2. **Synthetic Fault Generation** - Data augmentation for rare faults\n",
    "3. **Time Series Imputation** - Fill missing sensor data\n",
    "\n",
    "## Why Diffusion Models for Predictive Maintenance?\n",
    "\n",
    "| Advantage | Description |\n",
    "|-----------|-------------|\n",
    "| **High-Quality Generation** | Generate realistic synthetic fault data |\n",
    "| **Anomaly Detection** | Denoising error reveals anomalies |\n",
    "| **Few-Shot Learning** | Learn from limited fault examples |\n",
    "| **Uncertainty Quantification** | Probabilistic predictions |\n",
    "\n",
    "## Diffusion Process\n",
    "\n",
    "```\n",
    "Forward (add noise):   x_t = √(ᾱ_t) · x_0 + √(1-ᾱ_t) · ε\n",
    "Reverse (denoise):     x_{t-1} = model(x_t, t) \n",
    "Anomaly score:         ||x_0 - denoise(x_0 + noise)||\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score, roc_curve, classification_report\n",
    "import seaborn as sns\n",
    "import os\n",
    "import json\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "# Check TensorFlow availability\n",
    "try:\n",
    "    import tensorflow as tf\n",
    "    from tensorflow import keras\n",
    "    from tensorflow.keras import layers\n",
    "    print(f\"TensorFlow {tf.__version__} available\")\n",
    "    HAS_TF = True\n",
    "except ImportError:\n",
    "    print(\"TensorFlow not available\")\n",
    "    HAS_TF = False\n",
    "\n",
    "# Output directories\n",
    "DATA_DIR = '../data/diffusion'\n",
    "MODEL_DIR = '../models/diffusion'\n",
    "os.makedirs(DATA_DIR, exist_ok=True)\n",
    "os.makedirs(MODEL_DIR, exist_ok=True)\n",
    "\n",
    "print(\"Setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Diffusion Model Building Blocks\n",
    "\n",
    "### 1.1 Noise Schedule\n",
    "\n",
    "Define how noise is added during the forward process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_noise_schedule(T=1000, schedule='linear'):\n",
    "    \"\"\"\n",
    "    Generate noise schedule for diffusion.\n",
    "    \n",
    "    Args:\n",
    "        T: Number of diffusion steps\n",
    "        schedule: 'linear' or 'cosine'\n",
    "        \n",
    "    Returns:\n",
    "        betas, alphas, alpha_bar (cumulative product)\n",
    "    \"\"\"\n",
    "    if schedule == 'linear':\n",
    "        beta_start = 1e-4\n",
    "        beta_end = 0.02\n",
    "        betas = np.linspace(beta_start, beta_end, T)\n",
    "        \n",
    "    elif schedule == 'cosine':\n",
    "        # Cosine schedule from \"Improved Denoising Diffusion\"\n",
    "        s = 0.008\n",
    "        steps = np.arange(T + 1)\n",
    "        f = np.cos((steps / T + s) / (1 + s) * np.pi / 2) ** 2\n",
    "        alpha_bar = f / f[0]\n",
    "        betas = 1 - alpha_bar[1:] / alpha_bar[:-1]\n",
    "        betas = np.clip(betas, 0.0001, 0.999)\n",
    "    \n",
    "    alphas = 1 - betas\n",
    "    alpha_bar = np.cumprod(alphas)\n",
    "    \n",
    "    return betas.astype(np.float32), alphas.astype(np.float32), alpha_bar.astype(np.float32)\n",
    "\n",
    "# Visualize schedules\n",
    "T = 200  # Fewer steps for time series (vs 1000 for images)\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "for schedule in ['linear', 'cosine']:\n",
    "    betas, alphas, alpha_bar = get_noise_schedule(T, schedule)\n",
    "    \n",
    "    axes[0].plot(betas, label=schedule)\n",
    "    axes[1].plot(alphas, label=schedule)\n",
    "    axes[2].plot(alpha_bar, label=schedule)\n",
    "\n",
    "axes[0].set_title('Beta (noise added)')\n",
    "axes[1].set_title('Alpha (signal retained)')\n",
    "axes[2].set_title('Alpha Bar (cumulative signal)')\n",
    "\n",
    "for ax in axes:\n",
    "    ax.legend()\n",
    "    ax.set_xlabel('Timestep t')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{DATA_DIR}/noise_schedule.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if HAS_TF:\n",
    "    \n",
    "    class SinusoidalPositionEmbeddings(layers.Layer):\n",
    "        \"\"\"\n",
    "        Sinusoidal embeddings for diffusion timestep.\n",
    "        \"\"\"\n",
    "        \n",
    "        def __init__(self, dim, **kwargs):\n",
    "            super().__init__(**kwargs)\n",
    "            self.dim = dim\n",
    "            \n",
    "        def call(self, timesteps):\n",
    "            half_dim = self.dim // 2\n",
    "            embeddings = np.log(10000) / (half_dim - 1)\n",
    "            embeddings = tf.exp(tf.range(half_dim, dtype=tf.float32) * -embeddings)\n",
    "            embeddings = tf.cast(timesteps, tf.float32)[:, None] * embeddings[None, :]\n",
    "            embeddings = tf.concat([tf.sin(embeddings), tf.cos(embeddings)], axis=-1)\n",
    "            return embeddings\n",
    "        \n",
    "        def get_config(self):\n",
    "            config = super().get_config()\n",
    "            config.update({'dim': self.dim})\n",
    "            return config\n",
    "    \n",
    "    print(\"SinusoidalPositionEmbeddings defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if HAS_TF:\n",
    "    \n",
    "    class ResidualBlock1D(layers.Layer):\n",
    "        \"\"\"\n",
    "        1D Residual block for time series denoising.\n",
    "        \"\"\"\n",
    "        \n",
    "        def __init__(self, channels, kernel_size=3, **kwargs):\n",
    "            super().__init__(**kwargs)\n",
    "            self.channels = channels\n",
    "            self.kernel_size = kernel_size\n",
    "            \n",
    "        def build(self, input_shape):\n",
    "            self.conv1 = layers.Conv1D(self.channels, self.kernel_size, padding='same')\n",
    "            self.conv2 = layers.Conv1D(self.channels, self.kernel_size, padding='same')\n",
    "            self.norm1 = layers.LayerNormalization()\n",
    "            self.norm2 = layers.LayerNormalization()\n",
    "            self.time_proj = layers.Dense(self.channels)\n",
    "            \n",
    "            # Skip connection if channels change\n",
    "            in_channels = input_shape[-1]\n",
    "            if in_channels != self.channels:\n",
    "                self.skip_conv = layers.Conv1D(self.channels, 1)\n",
    "            else:\n",
    "                self.skip_conv = None\n",
    "                \n",
    "            super().build(input_shape)\n",
    "            \n",
    "        def call(self, inputs, time_emb=None):\n",
    "            x = inputs\n",
    "            \n",
    "            # First conv\n",
    "            h = self.norm1(x)\n",
    "            h = tf.nn.silu(h)\n",
    "            h = self.conv1(h)\n",
    "            \n",
    "            # Add time embedding\n",
    "            if time_emb is not None:\n",
    "                time_emb = self.time_proj(tf.nn.silu(time_emb))\n",
    "                h = h + time_emb[:, None, :]\n",
    "            \n",
    "            # Second conv\n",
    "            h = self.norm2(h)\n",
    "            h = tf.nn.silu(h)\n",
    "            h = self.conv2(h)\n",
    "            \n",
    "            # Skip connection\n",
    "            if self.skip_conv is not None:\n",
    "                x = self.skip_conv(x)\n",
    "            \n",
    "            return x + h\n",
    "        \n",
    "        def get_config(self):\n",
    "            config = super().get_config()\n",
    "            config.update({'channels': self.channels, 'kernel_size': self.kernel_size})\n",
    "            return config\n",
    "    \n",
    "    print(\"ResidualBlock1D defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if HAS_TF:\n",
    "    \n",
    "    def build_denoising_model(seq_length, n_features, time_embed_dim=64, channels=[64, 128, 64]):\n",
    "        \"\"\"\n",
    "        Build U-Net style denoising model for 1D time series.\n",
    "        \n",
    "        Predicts the noise ε given noisy input x_t and timestep t.\n",
    "        \"\"\"\n",
    "        # Inputs\n",
    "        x_input = keras.Input(shape=(seq_length, n_features), name='x_noisy')\n",
    "        t_input = keras.Input(shape=(), dtype=tf.int32, name='timestep')\n",
    "        \n",
    "        # Time embedding\n",
    "        time_emb = SinusoidalPositionEmbeddings(time_embed_dim)(t_input)\n",
    "        time_emb = layers.Dense(time_embed_dim * 2, activation='silu')(time_emb)\n",
    "        time_emb = layers.Dense(time_embed_dim)(time_emb)\n",
    "        \n",
    "        # Initial projection\n",
    "        x = layers.Conv1D(channels[0], 3, padding='same')(x_input)\n",
    "        \n",
    "        # Encoder (downsampling)\n",
    "        skips = []\n",
    "        for i, ch in enumerate(channels[:-1]):\n",
    "            x = ResidualBlock1D(ch)(x, time_emb)\n",
    "            x = ResidualBlock1D(ch)(x, time_emb)\n",
    "            skips.append(x)\n",
    "            # Downsample\n",
    "            x = layers.Conv1D(ch, 3, strides=2, padding='same')(x)\n",
    "        \n",
    "        # Middle\n",
    "        x = ResidualBlock1D(channels[-1])(x, time_emb)\n",
    "        x = ResidualBlock1D(channels[-1])(x, time_emb)\n",
    "        \n",
    "        # Decoder (upsampling)\n",
    "        for i, ch in enumerate(reversed(channels[:-1])):\n",
    "            # Upsample\n",
    "            x = layers.UpSampling1D(2)(x)\n",
    "            # Crop or pad to match skip size\n",
    "            skip = skips[-(i+1)]\n",
    "            x = layers.Conv1D(ch, 3, padding='same')(x)\n",
    "            # Resize if needed\n",
    "            if x.shape[1] != skip.shape[1]:\n",
    "                x = tf.image.resize(x[:, :, None, :], [skip.shape[1], 1])[:, :, 0, :]\n",
    "            x = layers.Concatenate()([x, skip])\n",
    "            x = ResidualBlock1D(ch)(x, time_emb)\n",
    "            x = ResidualBlock1D(ch)(x, time_emb)\n",
    "        \n",
    "        # Output: predict noise\n",
    "        x = layers.LayerNormalization()(x)\n",
    "        x = tf.nn.silu(x)\n",
    "        output = layers.Conv1D(n_features, 3, padding='same')(x)\n",
    "        \n",
    "        # Resize to exact input shape if needed\n",
    "        if output.shape[1] != seq_length:\n",
    "            output = tf.image.resize(output[:, :, None, :], [seq_length, 1])[:, :, 0, :]\n",
    "        \n",
    "        model = keras.Model(inputs=[x_input, t_input], outputs=output)\n",
    "        return model\n",
    "    \n",
    "    print(\"Denoising model builder defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Generate Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sensor_data(n_samples=1000, seq_length=128, n_features=4):\n",
    "    \"\"\"\n",
    "    Generate sensor time series data with normal and anomalous patterns.\n",
    "    \"\"\"\n",
    "    X_normal = []\n",
    "    X_anomaly = []\n",
    "    \n",
    "    # Normal samples (majority)\n",
    "    for _ in range(n_samples):\n",
    "        t = np.linspace(0, 4 * np.pi, seq_length)\n",
    "        \n",
    "        # Multi-sensor normal behavior\n",
    "        features = np.zeros((seq_length, n_features))\n",
    "        \n",
    "        # Feature 0: Vibration (periodic with small noise)\n",
    "        freq = np.random.uniform(0.8, 1.2)\n",
    "        features[:, 0] = np.sin(freq * t) + np.random.normal(0, 0.1, seq_length)\n",
    "        \n",
    "        # Feature 1: Temperature (slowly varying)\n",
    "        base_temp = np.random.uniform(0.4, 0.6)\n",
    "        features[:, 1] = base_temp + 0.1 * np.sin(0.2 * t) + np.random.normal(0, 0.02, seq_length)\n",
    "        \n",
    "        # Feature 2: Pressure (stable with noise)\n",
    "        features[:, 2] = 0.5 + np.random.normal(0, 0.05, seq_length)\n",
    "        \n",
    "        # Feature 3: Current (correlated with vibration)\n",
    "        features[:, 3] = 0.3 * features[:, 0] + 0.5 + np.random.normal(0, 0.08, seq_length)\n",
    "        \n",
    "        X_normal.append(features)\n",
    "    \n",
    "    # Anomalous samples (minority - for testing)\n",
    "    n_anomaly = n_samples // 5\n",
    "    \n",
    "    for _ in range(n_anomaly):\n",
    "        t = np.linspace(0, 4 * np.pi, seq_length)\n",
    "        features = np.zeros((seq_length, n_features))\n",
    "        \n",
    "        anomaly_type = np.random.choice(['spike', 'drift', 'noise', 'flatline'])\n",
    "        \n",
    "        # Start with normal pattern\n",
    "        freq = np.random.uniform(0.8, 1.2)\n",
    "        features[:, 0] = np.sin(freq * t) + np.random.normal(0, 0.1, seq_length)\n",
    "        features[:, 1] = 0.5 + 0.1 * np.sin(0.2 * t) + np.random.normal(0, 0.02, seq_length)\n",
    "        features[:, 2] = 0.5 + np.random.normal(0, 0.05, seq_length)\n",
    "        features[:, 3] = 0.3 * features[:, 0] + 0.5 + np.random.normal(0, 0.08, seq_length)\n",
    "        \n",
    "        # Apply anomaly\n",
    "        if anomaly_type == 'spike':\n",
    "            # Random spikes\n",
    "            spike_locs = np.random.choice(seq_length, size=np.random.randint(3, 8), replace=False)\n",
    "            features[spike_locs, 0] += np.random.uniform(2, 4, len(spike_locs))\n",
    "            \n",
    "        elif anomaly_type == 'drift':\n",
    "            # Gradual drift\n",
    "            drift = np.linspace(0, np.random.uniform(0.5, 1.5), seq_length)\n",
    "            features[:, 1] += drift\n",
    "            \n",
    "        elif anomaly_type == 'noise':\n",
    "            # Increased noise\n",
    "            features[:, 0] += np.random.normal(0, 0.8, seq_length)\n",
    "            features[:, 2] += np.random.normal(0, 0.3, seq_length)\n",
    "            \n",
    "        elif anomaly_type == 'flatline':\n",
    "            # Sensor stuck\n",
    "            start = np.random.randint(0, seq_length // 2)\n",
    "            features[start:, 2] = features[start, 2]\n",
    "        \n",
    "        X_anomaly.append(features)\n",
    "    \n",
    "    return np.array(X_normal), np.array(X_anomaly)\n",
    "\n",
    "# Generate data\n",
    "print(\"Generating sensor data...\")\n",
    "X_normal, X_anomaly = generate_sensor_data(n_samples=1500, seq_length=128, n_features=4)\n",
    "print(f\"Normal samples: {X_normal.shape}\")\n",
    "print(f\"Anomaly samples: {X_anomaly.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize samples\n",
    "fig, axes = plt.subplots(2, 4, figsize=(16, 6))\n",
    "\n",
    "# Normal samples\n",
    "for i in range(4):\n",
    "    axes[0, i].plot(X_normal[i, :, 0], label='Vibration', alpha=0.7)\n",
    "    axes[0, i].plot(X_normal[i, :, 1], label='Temp', alpha=0.7)\n",
    "    axes[0, i].set_title(f'Normal Sample {i+1}')\n",
    "    if i == 0:\n",
    "        axes[0, i].legend(fontsize=8)\n",
    "\n",
    "# Anomaly samples\n",
    "for i in range(4):\n",
    "    axes[1, i].plot(X_anomaly[i, :, 0], label='Vibration', alpha=0.7)\n",
    "    axes[1, i].plot(X_anomaly[i, :, 1], label='Temp', alpha=0.7)\n",
    "    axes[1, i].set_title(f'Anomaly Sample {i+1}')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{DATA_DIR}/sample_data.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Diffusion Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if HAS_TF:\n",
    "    \n",
    "    class DiffusionModel:\n",
    "        \"\"\"\n",
    "        Complete Diffusion Model for time series.\n",
    "        \"\"\"\n",
    "        \n",
    "        def __init__(self, seq_length, n_features, T=200):\n",
    "            self.seq_length = seq_length\n",
    "            self.n_features = n_features\n",
    "            self.T = T\n",
    "            \n",
    "            # Noise schedule\n",
    "            self.betas, self.alphas, self.alpha_bar = get_noise_schedule(T, 'cosine')\n",
    "            \n",
    "            # Build denoising model\n",
    "            self.model = build_denoising_model(\n",
    "                seq_length=seq_length,\n",
    "                n_features=n_features,\n",
    "                channels=[32, 64, 32]  # Smaller for faster training\n",
    "            )\n",
    "            \n",
    "            self.optimizer = keras.optimizers.Adam(1e-4)\n",
    "            \n",
    "        def forward_diffusion(self, x_0, t):\n",
    "            \"\"\"\n",
    "            Add noise according to schedule.\n",
    "            \n",
    "            x_t = √(ᾱ_t) · x_0 + √(1-ᾱ_t) · ε\n",
    "            \"\"\"\n",
    "            noise = tf.random.normal(tf.shape(x_0))\n",
    "            \n",
    "            # Get alpha_bar for each sample's timestep\n",
    "            alpha_bar_t = tf.gather(self.alpha_bar, t)\n",
    "            alpha_bar_t = tf.reshape(alpha_bar_t, [-1, 1, 1])\n",
    "            \n",
    "            # Apply forward diffusion\n",
    "            x_t = tf.sqrt(alpha_bar_t) * x_0 + tf.sqrt(1 - alpha_bar_t) * noise\n",
    "            \n",
    "            return x_t, noise\n",
    "        \n",
    "        @tf.function\n",
    "        def train_step(self, x_0):\n",
    "            \"\"\"\n",
    "            Single training step.\n",
    "            \"\"\"\n",
    "            batch_size = tf.shape(x_0)[0]\n",
    "            \n",
    "            # Sample random timesteps\n",
    "            t = tf.random.uniform([batch_size], 0, self.T, dtype=tf.int32)\n",
    "            \n",
    "            # Forward diffusion\n",
    "            x_t, noise = self.forward_diffusion(x_0, t)\n",
    "            \n",
    "            with tf.GradientTape() as tape:\n",
    "                # Predict noise\n",
    "                noise_pred = self.model([x_t, t], training=True)\n",
    "                \n",
    "                # MSE loss\n",
    "                loss = tf.reduce_mean((noise - noise_pred) ** 2)\n",
    "            \n",
    "            # Update weights\n",
    "            gradients = tape.gradient(loss, self.model.trainable_variables)\n",
    "            self.optimizer.apply_gradients(zip(gradients, self.model.trainable_variables))\n",
    "            \n",
    "            return loss\n",
    "        \n",
    "        def train(self, X, epochs=50, batch_size=32):\n",
    "            \"\"\"\n",
    "            Train the diffusion model.\n",
    "            \"\"\"\n",
    "            dataset = tf.data.Dataset.from_tensor_slices(X.astype(np.float32))\n",
    "            dataset = dataset.shuffle(len(X)).batch(batch_size)\n",
    "            \n",
    "            losses = []\n",
    "            \n",
    "            for epoch in range(epochs):\n",
    "                epoch_loss = 0\n",
    "                n_batches = 0\n",
    "                \n",
    "                for batch in dataset:\n",
    "                    loss = self.train_step(batch)\n",
    "                    epoch_loss += loss.numpy()\n",
    "                    n_batches += 1\n",
    "                \n",
    "                avg_loss = epoch_loss / n_batches\n",
    "                losses.append(avg_loss)\n",
    "                \n",
    "                if (epoch + 1) % 10 == 0:\n",
    "                    print(f\"Epoch {epoch+1}/{epochs}, Loss: {avg_loss:.6f}\")\n",
    "            \n",
    "            return losses\n",
    "        \n",
    "        def denoise(self, x_noisy, t):\n",
    "            \"\"\"\n",
    "            Single denoising step.\n",
    "            \"\"\"\n",
    "            noise_pred = self.model([x_noisy, t], training=False)\n",
    "            \n",
    "            alpha_t = tf.gather(self.alphas, t)\n",
    "            alpha_bar_t = tf.gather(self.alpha_bar, t)\n",
    "            \n",
    "            alpha_t = tf.reshape(alpha_t, [-1, 1, 1])\n",
    "            alpha_bar_t = tf.reshape(alpha_bar_t, [-1, 1, 1])\n",
    "            \n",
    "            # Predict x_0\n",
    "            x_0_pred = (x_noisy - tf.sqrt(1 - alpha_bar_t) * noise_pred) / tf.sqrt(alpha_bar_t)\n",
    "            \n",
    "            return x_0_pred, noise_pred\n",
    "        \n",
    "        def compute_anomaly_score(self, x, n_steps=50):\n",
    "            \"\"\"\n",
    "            Compute anomaly score based on reconstruction error.\n",
    "            \n",
    "            1. Add noise at timestep t\n",
    "            2. Denoise back\n",
    "            3. Compare to original\n",
    "            \"\"\"\n",
    "            # Use middle timestep for balanced noise\n",
    "            t = np.full(len(x), n_steps, dtype=np.int32)\n",
    "            \n",
    "            x_tf = tf.cast(x, tf.float32)\n",
    "            t_tf = tf.cast(t, tf.int32)\n",
    "            \n",
    "            # Add noise\n",
    "            x_noisy, _ = self.forward_diffusion(x_tf, t_tf)\n",
    "            \n",
    "            # Denoise\n",
    "            x_recon, _ = self.denoise(x_noisy, t_tf)\n",
    "            \n",
    "            # Reconstruction error\n",
    "            error = tf.reduce_mean((x_tf - x_recon) ** 2, axis=[1, 2])\n",
    "            \n",
    "            return error.numpy()\n",
    "        \n",
    "        def generate(self, n_samples=10):\n",
    "            \"\"\"\n",
    "            Generate new samples via reverse diffusion.\n",
    "            \"\"\"\n",
    "            # Start from pure noise\n",
    "            x = tf.random.normal([n_samples, self.seq_length, self.n_features])\n",
    "            \n",
    "            # Reverse diffusion\n",
    "            for t in reversed(range(self.T)):\n",
    "                t_batch = tf.fill([n_samples], t)\n",
    "                \n",
    "                noise_pred = self.model([x, t_batch], training=False)\n",
    "                \n",
    "                alpha_t = self.alphas[t]\n",
    "                alpha_bar_t = self.alpha_bar[t]\n",
    "                beta_t = self.betas[t]\n",
    "                \n",
    "                # Compute mean\n",
    "                coef1 = 1 / np.sqrt(alpha_t)\n",
    "                coef2 = beta_t / np.sqrt(1 - alpha_bar_t)\n",
    "                mean = coef1 * (x - coef2 * noise_pred)\n",
    "                \n",
    "                # Add noise (except at t=0)\n",
    "                if t > 0:\n",
    "                    noise = tf.random.normal(tf.shape(x))\n",
    "                    sigma = np.sqrt(beta_t)\n",
    "                    x = mean + sigma * noise\n",
    "                else:\n",
    "                    x = mean\n",
    "            \n",
    "            return x.numpy()\n",
    "    \n",
    "    print(\"DiffusionModel class defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if HAS_TF:\n",
    "    # Normalize data\n",
    "    scaler = StandardScaler()\n",
    "    X_normal_flat = X_normal.reshape(-1, X_normal.shape[-1])\n",
    "    scaler.fit(X_normal_flat)\n",
    "    \n",
    "    X_normal_scaled = np.array([scaler.transform(x) for x in X_normal])\n",
    "    X_anomaly_scaled = np.array([scaler.transform(x) for x in X_anomaly])\n",
    "    \n",
    "    # Train on normal data only\n",
    "    X_train, X_val = train_test_split(X_normal_scaled, test_size=0.15, random_state=42)\n",
    "    \n",
    "    print(f\"Training samples: {len(X_train)}\")\n",
    "    print(f\"Validation samples: {len(X_val)}\")\n",
    "    \n",
    "    # Build and train model\n",
    "    diffusion = DiffusionModel(\n",
    "        seq_length=128,\n",
    "        n_features=4,\n",
    "        T=100  # Fewer steps for faster training\n",
    "    )\n",
    "    \n",
    "    diffusion.model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if HAS_TF:\n",
    "    print(\"Training diffusion model...\")\n",
    "    print(\"(This may take a few minutes)\")\n",
    "    \n",
    "    losses = diffusion.train(X_train, epochs=30, batch_size=32)\n",
    "    \n",
    "    # Plot training loss\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plt.plot(losses)\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Diffusion Model Training')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.savefig(f'{MODEL_DIR}/training_loss.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Anomaly Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if HAS_TF:\n",
    "    # Compute anomaly scores\n",
    "    print(\"Computing anomaly scores...\")\n",
    "    \n",
    "    scores_normal = diffusion.compute_anomaly_score(X_val, n_steps=30)\n",
    "    scores_anomaly = diffusion.compute_anomaly_score(X_anomaly_scaled, n_steps=30)\n",
    "    \n",
    "    print(f\"Normal scores: mean={scores_normal.mean():.4f}, std={scores_normal.std():.4f}\")\n",
    "    print(f\"Anomaly scores: mean={scores_anomaly.mean():.4f}, std={scores_anomaly.std():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if HAS_TF:\n",
    "    # Visualize results\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "    \n",
    "    # Distribution of scores\n",
    "    axes[0].hist(scores_normal, bins=30, alpha=0.7, label='Normal', color='green')\n",
    "    axes[0].hist(scores_anomaly, bins=30, alpha=0.7, label='Anomaly', color='red')\n",
    "    axes[0].set_xlabel('Anomaly Score (Reconstruction Error)')\n",
    "    axes[0].set_ylabel('Count')\n",
    "    axes[0].set_title('Score Distribution')\n",
    "    axes[0].legend()\n",
    "    \n",
    "    # ROC curve\n",
    "    labels = np.concatenate([np.zeros(len(scores_normal)), np.ones(len(scores_anomaly))])\n",
    "    scores = np.concatenate([scores_normal, scores_anomaly])\n",
    "    \n",
    "    fpr, tpr, thresholds = roc_curve(labels, scores)\n",
    "    auc_score = roc_auc_score(labels, scores)\n",
    "    \n",
    "    axes[1].plot(fpr, tpr, 'b-', linewidth=2, label=f'AUC = {auc_score:.3f}')\n",
    "    axes[1].plot([0, 1], [0, 1], 'k--', alpha=0.3)\n",
    "    axes[1].set_xlabel('False Positive Rate')\n",
    "    axes[1].set_ylabel('True Positive Rate')\n",
    "    axes[1].set_title('ROC Curve')\n",
    "    axes[1].legend()\n",
    "    \n",
    "    # Box plot\n",
    "    axes[2].boxplot([scores_normal, scores_anomaly], labels=['Normal', 'Anomaly'])\n",
    "    axes[2].set_ylabel('Anomaly Score')\n",
    "    axes[2].set_title('Score Comparison')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{MODEL_DIR}/anomaly_detection_results.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"\\nDiffusion Anomaly Detection AUC: {auc_score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Synthetic Fault Generation (Data Augmentation)\n",
    "\n",
    "Use diffusion to generate realistic fault patterns for training classifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if HAS_TF:\n",
    "    \n",
    "    class ConditionalDiffusionModel(DiffusionModel):\n",
    "        \"\"\"\n",
    "        Conditional Diffusion for generating specific fault types.\n",
    "        \"\"\"\n",
    "        \n",
    "        def __init__(self, seq_length, n_features, n_classes, T=200):\n",
    "            self.n_classes = n_classes\n",
    "            super().__init__(seq_length, n_features, T)\n",
    "            \n",
    "            # Rebuild with conditioning\n",
    "            self.model = self._build_conditional_model()\n",
    "            self.optimizer = keras.optimizers.Adam(1e-4)\n",
    "            \n",
    "        def _build_conditional_model(self):\n",
    "            \"\"\"\n",
    "            Build denoising model with class conditioning.\n",
    "            \"\"\"\n",
    "            # Inputs\n",
    "            x_input = keras.Input(shape=(self.seq_length, self.n_features), name='x_noisy')\n",
    "            t_input = keras.Input(shape=(), dtype=tf.int32, name='timestep')\n",
    "            c_input = keras.Input(shape=(), dtype=tf.int32, name='class')  # Class label\n",
    "            \n",
    "            # Time embedding\n",
    "            time_emb = SinusoidalPositionEmbeddings(64)(t_input)\n",
    "            time_emb = layers.Dense(128, activation='silu')(time_emb)\n",
    "            time_emb = layers.Dense(64)(time_emb)\n",
    "            \n",
    "            # Class embedding\n",
    "            class_emb = layers.Embedding(self.n_classes, 64)(c_input)\n",
    "            \n",
    "            # Combine embeddings\n",
    "            cond_emb = time_emb + class_emb\n",
    "            \n",
    "            # Denoising network\n",
    "            x = layers.Conv1D(32, 3, padding='same')(x_input)\n",
    "            \n",
    "            # Add conditioning via FiLM (Feature-wise Linear Modulation)\n",
    "            gamma = layers.Dense(32)(cond_emb)\n",
    "            beta = layers.Dense(32)(cond_emb)\n",
    "            x = x * gamma[:, None, :] + beta[:, None, :]\n",
    "            \n",
    "            x = ResidualBlock1D(64)(x, cond_emb)\n",
    "            x = ResidualBlock1D(64)(x, cond_emb)\n",
    "            x = ResidualBlock1D(32)(x, cond_emb)\n",
    "            \n",
    "            x = layers.LayerNormalization()(x)\n",
    "            x = tf.nn.silu(x)\n",
    "            output = layers.Conv1D(self.n_features, 3, padding='same')(x)\n",
    "            \n",
    "            model = keras.Model(inputs=[x_input, t_input, c_input], outputs=output)\n",
    "            return model\n",
    "        \n",
    "        @tf.function\n",
    "        def train_step_conditional(self, x_0, labels):\n",
    "            batch_size = tf.shape(x_0)[0]\n",
    "            t = tf.random.uniform([batch_size], 0, self.T, dtype=tf.int32)\n",
    "            \n",
    "            x_t, noise = self.forward_diffusion(x_0, t)\n",
    "            \n",
    "            with tf.GradientTape() as tape:\n",
    "                noise_pred = self.model([x_t, t, labels], training=True)\n",
    "                loss = tf.reduce_mean((noise - noise_pred) ** 2)\n",
    "            \n",
    "            gradients = tape.gradient(loss, self.model.trainable_variables)\n",
    "            self.optimizer.apply_gradients(zip(gradients, self.model.trainable_variables))\n",
    "            \n",
    "            return loss\n",
    "        \n",
    "        def train_conditional(self, X, labels, epochs=50, batch_size=32):\n",
    "            dataset = tf.data.Dataset.from_tensor_slices(\n",
    "                (X.astype(np.float32), labels.astype(np.int32))\n",
    "            )\n",
    "            dataset = dataset.shuffle(len(X)).batch(batch_size)\n",
    "            \n",
    "            losses = []\n",
    "            \n",
    "            for epoch in range(epochs):\n",
    "                epoch_loss = 0\n",
    "                n_batches = 0\n",
    "                \n",
    "                for x_batch, label_batch in dataset:\n",
    "                    loss = self.train_step_conditional(x_batch, label_batch)\n",
    "                    epoch_loss += loss.numpy()\n",
    "                    n_batches += 1\n",
    "                \n",
    "                avg_loss = epoch_loss / n_batches\n",
    "                losses.append(avg_loss)\n",
    "                \n",
    "                if (epoch + 1) % 10 == 0:\n",
    "                    print(f\"Epoch {epoch+1}/{epochs}, Loss: {avg_loss:.6f}\")\n",
    "            \n",
    "            return losses\n",
    "        \n",
    "        def generate_class(self, class_label, n_samples=10):\n",
    "            \"\"\"\n",
    "            Generate samples of a specific class.\n",
    "            \"\"\"\n",
    "            x = tf.random.normal([n_samples, self.seq_length, self.n_features])\n",
    "            labels = tf.fill([n_samples], class_label)\n",
    "            \n",
    "            for t in reversed(range(self.T)):\n",
    "                t_batch = tf.fill([n_samples], t)\n",
    "                \n",
    "                noise_pred = self.model([x, t_batch, labels], training=False)\n",
    "                \n",
    "                alpha_t = self.alphas[t]\n",
    "                alpha_bar_t = self.alpha_bar[t]\n",
    "                beta_t = self.betas[t]\n",
    "                \n",
    "                coef1 = 1 / np.sqrt(alpha_t)\n",
    "                coef2 = beta_t / np.sqrt(1 - alpha_bar_t)\n",
    "                mean = coef1 * (x - coef2 * noise_pred)\n",
    "                \n",
    "                if t > 0:\n",
    "                    noise = tf.random.normal(tf.shape(x))\n",
    "                    sigma = np.sqrt(beta_t)\n",
    "                    x = mean + sigma * noise\n",
    "                else:\n",
    "                    x = mean\n",
    "            \n",
    "            return x.numpy()\n",
    "    \n",
    "    print(\"ConditionalDiffusionModel defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create labeled dataset with different fault types\n",
    "def generate_labeled_faults(n_per_class=200, seq_length=128, n_features=4):\n",
    "    \"\"\"\n",
    "    Generate labeled data for conditional diffusion.\n",
    "    \"\"\"\n",
    "    X = []\n",
    "    y = []\n",
    "    \n",
    "    fault_types = ['normal', 'spike', 'drift', 'noise_burst', 'degradation']\n",
    "    \n",
    "    for class_idx, fault in enumerate(fault_types):\n",
    "        for _ in range(n_per_class):\n",
    "            t = np.linspace(0, 4 * np.pi, seq_length)\n",
    "            features = np.zeros((seq_length, n_features))\n",
    "            \n",
    "            # Base signal\n",
    "            freq = np.random.uniform(0.8, 1.2)\n",
    "            features[:, 0] = np.sin(freq * t) + np.random.normal(0, 0.1, seq_length)\n",
    "            features[:, 1] = 0.5 + 0.1 * np.sin(0.2 * t) + np.random.normal(0, 0.02, seq_length)\n",
    "            features[:, 2] = 0.5 + np.random.normal(0, 0.05, seq_length)\n",
    "            features[:, 3] = 0.3 * features[:, 0] + 0.5 + np.random.normal(0, 0.08, seq_length)\n",
    "            \n",
    "            if fault == 'spike':\n",
    "                n_spikes = np.random.randint(3, 10)\n",
    "                spike_locs = np.random.choice(seq_length, n_spikes, replace=False)\n",
    "                spike_heights = np.random.uniform(2, 5, n_spikes)\n",
    "                features[spike_locs, 0] += spike_heights\n",
    "                \n",
    "            elif fault == 'drift':\n",
    "                drift_rate = np.random.uniform(0.3, 1.0)\n",
    "                drift = drift_rate * np.linspace(0, 1, seq_length)\n",
    "                features[:, 1] += drift\n",
    "                features[:, 3] += drift * 0.5\n",
    "                \n",
    "            elif fault == 'noise_burst':\n",
    "                burst_start = np.random.randint(0, seq_length // 2)\n",
    "                burst_len = np.random.randint(20, 50)\n",
    "                burst_end = min(burst_start + burst_len, seq_length)\n",
    "                features[burst_start:burst_end, 0] += np.random.normal(0, 1, burst_end - burst_start)\n",
    "                features[burst_start:burst_end, 2] += np.random.normal(0, 0.5, burst_end - burst_start)\n",
    "                \n",
    "            elif fault == 'degradation':\n",
    "                # Gradual amplitude increase + frequency shift\n",
    "                degradation = np.linspace(1, 2, seq_length)\n",
    "                features[:, 0] *= degradation\n",
    "                freq_shift = np.linspace(0, 0.5, seq_length)\n",
    "                features[:, 0] += 0.3 * np.sin((freq + freq_shift) * t)\n",
    "            \n",
    "            X.append(features)\n",
    "            y.append(class_idx)\n",
    "    \n",
    "    return np.array(X), np.array(y), fault_types\n",
    "\n",
    "# Generate data\n",
    "print(\"Generating labeled fault data...\")\n",
    "X_labeled, y_labeled, fault_types = generate_labeled_faults(n_per_class=150)\n",
    "print(f\"Generated: X={X_labeled.shape}, y={y_labeled.shape}\")\n",
    "print(f\"Classes: {fault_types}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if HAS_TF:\n",
    "    # Normalize\n",
    "    scaler_cond = StandardScaler()\n",
    "    X_labeled_flat = X_labeled.reshape(-1, X_labeled.shape[-1])\n",
    "    scaler_cond.fit(X_labeled_flat)\n",
    "    \n",
    "    X_labeled_scaled = np.array([scaler_cond.transform(x) for x in X_labeled])\n",
    "    \n",
    "    # Build and train conditional model\n",
    "    cond_diffusion = ConditionalDiffusionModel(\n",
    "        seq_length=128,\n",
    "        n_features=4,\n",
    "        n_classes=len(fault_types),\n",
    "        T=100\n",
    "    )\n",
    "    \n",
    "    print(\"Training conditional diffusion model...\")\n",
    "    cond_losses = cond_diffusion.train_conditional(\n",
    "        X_labeled_scaled, y_labeled, \n",
    "        epochs=30, \n",
    "        batch_size=32\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if HAS_TF:\n",
    "    # Generate synthetic samples for each class\n",
    "    print(\"\\nGenerating synthetic samples...\")\n",
    "    \n",
    "    fig, axes = plt.subplots(len(fault_types), 2, figsize=(14, 3 * len(fault_types)))\n",
    "    \n",
    "    for class_idx, fault_name in enumerate(fault_types):\n",
    "        # Real sample\n",
    "        real_idx = np.where(y_labeled == class_idx)[0][0]\n",
    "        real_sample = X_labeled_scaled[real_idx]\n",
    "        \n",
    "        # Generated sample\n",
    "        generated = cond_diffusion.generate_class(class_idx, n_samples=1)[0]\n",
    "        \n",
    "        # Plot real\n",
    "        axes[class_idx, 0].plot(real_sample[:, 0], label='Vibration', alpha=0.7)\n",
    "        axes[class_idx, 0].plot(real_sample[:, 1], label='Temperature', alpha=0.7)\n",
    "        axes[class_idx, 0].set_title(f'{fault_name} - Real')\n",
    "        axes[class_idx, 0].set_ylabel('Value')\n",
    "        if class_idx == 0:\n",
    "            axes[class_idx, 0].legend()\n",
    "        \n",
    "        # Plot generated\n",
    "        axes[class_idx, 1].plot(generated[:, 0], label='Vibration', alpha=0.7)\n",
    "        axes[class_idx, 1].plot(generated[:, 1], label='Temperature', alpha=0.7)\n",
    "        axes[class_idx, 1].set_title(f'{fault_name} - Generated')\n",
    "    \n",
    "    axes[-1, 0].set_xlabel('Time Step')\n",
    "    axes[-1, 1].set_xlabel('Time Step')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{MODEL_DIR}/synthetic_faults.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nSynthetic fault patterns generated!\")\n",
    "    print(\"These can be used to augment training data for classifiers.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Time Series Imputation\n",
    "\n",
    "Use diffusion to fill missing sensor values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if HAS_TF:\n",
    "    \n",
    "    def impute_missing_values(diffusion_model, x_with_missing, mask, n_iterations=10):\n",
    "        \"\"\"\n",
    "        Impute missing values using diffusion.\n",
    "        \n",
    "        Args:\n",
    "            x_with_missing: Data with missing values (can be any value where mask=0)\n",
    "            mask: Binary mask (1=observed, 0=missing)\n",
    "            n_iterations: Number of refinement iterations\n",
    "        \"\"\"\n",
    "        x = tf.cast(x_with_missing, tf.float32)\n",
    "        mask = tf.cast(mask, tf.float32)\n",
    "        \n",
    "        for i in range(n_iterations):\n",
    "            # Add small noise\n",
    "            t = 20  # Low noise level\n",
    "            t_batch = tf.fill([len(x)], t)\n",
    "            \n",
    "            # Only add noise to missing regions\n",
    "            noise = tf.random.normal(tf.shape(x)) * 0.1\n",
    "            x_noisy = x + noise * (1 - mask)\n",
    "            \n",
    "            # Denoise\n",
    "            x_denoised, _ = diffusion_model.denoise(x_noisy, t_batch)\n",
    "            \n",
    "            # Keep observed values, update missing\n",
    "            x = mask * x + (1 - mask) * x_denoised\n",
    "        \n",
    "        return x.numpy()\n",
    "    \n",
    "    # Create data with missing values\n",
    "    test_sample = X_normal_scaled[0:5].copy()\n",
    "    \n",
    "    # Create missing mask (simulate sensor dropout)\n",
    "    mask = np.ones_like(test_sample)\n",
    "    \n",
    "    for i in range(len(test_sample)):\n",
    "        # Random missing segments\n",
    "        n_missing = np.random.randint(2, 5)\n",
    "        for _ in range(n_missing):\n",
    "            start = np.random.randint(0, 100)\n",
    "            length = np.random.randint(10, 30)\n",
    "            feature = np.random.randint(0, 4)\n",
    "            mask[i, start:start+length, feature] = 0\n",
    "    \n",
    "    # Zero out missing values\n",
    "    test_with_missing = test_sample * mask\n",
    "    \n",
    "    # Impute\n",
    "    print(\"Imputing missing values...\")\n",
    "    imputed = impute_missing_values(diffusion, test_with_missing, mask, n_iterations=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if HAS_TF:\n",
    "    # Visualize imputation\n",
    "    fig, axes = plt.subplots(3, 1, figsize=(14, 8))\n",
    "    \n",
    "    sample_idx = 0\n",
    "    feature_idx = 0\n",
    "    \n",
    "    # Original\n",
    "    axes[0].plot(test_sample[sample_idx, :, feature_idx], 'b-', linewidth=2)\n",
    "    axes[0].set_title('Original Signal')\n",
    "    axes[0].set_ylabel('Value')\n",
    "    \n",
    "    # With missing (show gaps)\n",
    "    missing_signal = test_with_missing[sample_idx, :, feature_idx].copy()\n",
    "    missing_signal[mask[sample_idx, :, feature_idx] == 0] = np.nan\n",
    "    axes[1].plot(missing_signal, 'b-', linewidth=2)\n",
    "    # Highlight missing regions\n",
    "    missing_regions = np.where(mask[sample_idx, :, feature_idx] == 0)[0]\n",
    "    if len(missing_regions) > 0:\n",
    "        axes[1].fill_between(range(len(missing_signal)), \n",
    "                            axes[1].get_ylim()[0], axes[1].get_ylim()[1],\n",
    "                            where=(mask[sample_idx, :, feature_idx] == 0),\n",
    "                            alpha=0.3, color='red', label='Missing')\n",
    "    axes[1].set_title('Signal with Missing Data')\n",
    "    axes[1].set_ylabel('Value')\n",
    "    axes[1].legend()\n",
    "    \n",
    "    # Imputed\n",
    "    axes[2].plot(test_sample[sample_idx, :, feature_idx], 'b-', linewidth=2, label='Original', alpha=0.5)\n",
    "    axes[2].plot(imputed[sample_idx, :, feature_idx], 'g--', linewidth=2, label='Imputed')\n",
    "    axes[2].set_title('Imputed Signal')\n",
    "    axes[2].set_xlabel('Time Step')\n",
    "    axes[2].set_ylabel('Value')\n",
    "    axes[2].legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{MODEL_DIR}/imputation_results.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    # Compute imputation error\n",
    "    missing_mask = (mask == 0)\n",
    "    if missing_mask.sum() > 0:\n",
    "        imputation_error = np.abs(test_sample[missing_mask] - imputed[missing_mask]).mean()\n",
    "        print(f\"\\nMean Imputation Error (on missing values): {imputation_error:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Save Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if HAS_TF:\n",
    "    # Save models\n",
    "    diffusion.model.save(f'{MODEL_DIR}/diffusion_anomaly_detector.keras')\n",
    "    cond_diffusion.model.save(f'{MODEL_DIR}/conditional_diffusion.keras')\n",
    "    \n",
    "    # Save noise schedule\n",
    "    np.savez(\n",
    "        f'{MODEL_DIR}/noise_schedule.npz',\n",
    "        betas=diffusion.betas,\n",
    "        alphas=diffusion.alphas,\n",
    "        alpha_bar=diffusion.alpha_bar\n",
    "    )\n",
    "    \n",
    "    # Save metadata\n",
    "    metadata = {\n",
    "        'model_type': 'Denoising Diffusion Probabilistic Model',\n",
    "        'models': {\n",
    "            'anomaly_detector': {\n",
    "                'file': 'diffusion_anomaly_detector.keras',\n",
    "                'T': diffusion.T,\n",
    "                'seq_length': diffusion.seq_length,\n",
    "                'n_features': diffusion.n_features,\n",
    "                'auc': float(auc_score)\n",
    "            },\n",
    "            'conditional': {\n",
    "                'file': 'conditional_diffusion.keras',\n",
    "                'classes': fault_types\n",
    "            }\n",
    "        },\n",
    "        'applications': [\n",
    "            'Anomaly detection via reconstruction error',\n",
    "            'Synthetic fault generation for data augmentation',\n",
    "            'Time series imputation for missing values'\n",
    "        ],\n",
    "        'advantages': [\n",
    "            'High-quality synthetic data generation',\n",
    "            'Works with limited labeled data',\n",
    "            'Probabilistic uncertainty quantification'\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    with open(f'{MODEL_DIR}/diffusion_metadata.json', 'w') as f:\n",
    "        json.dump(metadata, f, indent=2)\n",
    "    \n",
    "    print(f\"\\nModels saved to {MODEL_DIR}/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Node-RED Integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_red_code = '''\n",
    "// Node-RED Function: Diffusion-based Anomaly Detection\n",
    "\n",
    "const SEQ_LENGTH = 128;\n",
    "const N_FEATURES = 4;\n",
    "const NOISE_STEP = 30;  // How much noise to add for reconstruction\n",
    "\n",
    "// Collect sensor readings\n",
    "if (!context.buffer) {\n",
    "    context.buffer = [];\n",
    "}\n",
    "\n",
    "context.buffer.push([\n",
    "    msg.payload.vibration,\n",
    "    msg.payload.temperature,\n",
    "    msg.payload.pressure,\n",
    "    msg.payload.current\n",
    "]);\n",
    "\n",
    "if (context.buffer.length > SEQ_LENGTH) {\n",
    "    context.buffer.shift();\n",
    "}\n",
    "\n",
    "if (context.buffer.length < SEQ_LENGTH) {\n",
    "    msg.payload = { status: \"collecting\", samples: context.buffer.length };\n",
    "    return msg;\n",
    "}\n",
    "\n",
    "// Prepare for model\n",
    "// The diffusion model will:\n",
    "// 1. Add noise to the input\n",
    "// 2. Attempt to denoise\n",
    "// 3. Compare reconstruction to original\n",
    "\n",
    "msg.payload = {\n",
    "    input: [context.buffer],\n",
    "    noise_step: NOISE_STEP,\n",
    "    model: \"diffusion_anomaly\",\n",
    "    \n",
    "    // Thresholds for alerting\n",
    "    warning_threshold: 0.5,\n",
    "    critical_threshold: 1.0\n",
    "};\n",
    "\n",
    "return msg;\n",
    "''';\n",
    "\n",
    "print(\"Node-RED Integration Code:\")\n",
    "print(\"=\" * 50)\n",
    "print(node_red_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrated **Diffusion Models** for Predictive Maintenance:\n",
    "\n",
    "### Key Concepts:\n",
    "\n",
    "| Component | Purpose |\n",
    "|-----------|--------|\n",
    "| **Forward Diffusion** | Gradually add noise to data |\n",
    "| **Reverse Diffusion** | Learn to denoise step by step |\n",
    "| **Anomaly Score** | Reconstruction error indicates anomalies |\n",
    "| **Conditional Generation** | Generate specific fault types |\n",
    "\n",
    "### Applications:\n",
    "\n",
    "1. **Anomaly Detection** - High reconstruction error = anomaly\n",
    "2. **Data Augmentation** - Generate synthetic faults for training\n",
    "3. **Imputation** - Fill missing sensor values\n",
    "4. **Uncertainty Quantification** - Probabilistic predictions\n",
    "\n",
    "### When to Use Diffusion Models:\n",
    "\n",
    "- Limited fault data (few-shot learning)\n",
    "- Need to generate realistic synthetic data\n",
    "- Want probabilistic anomaly scores\n",
    "- Dealing with missing or corrupted sensor data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
