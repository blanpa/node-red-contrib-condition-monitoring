{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graph Neural Networks for Predictive Maintenance\n",
    "\n",
    "This notebook implements **Graph Neural Networks (GNNs)** for condition monitoring of multi-component systems:\n",
    "\n",
    "1. **Sensor Network Modeling** - Capture sensor interdependencies\n",
    "2. **Graph Attention for Fault Detection** - Attention-weighted neighbor aggregation\n",
    "3. **Spatio-Temporal GNN** - Combine spatial (graph) and temporal patterns\n",
    "4. **Multi-Machine Health Monitoring** - Factory-level predictions\n",
    "\n",
    "## Why GNNs for Predictive Maintenance?\n",
    "\n",
    "| Advantage | Description |\n",
    "|-----------|-------------|\n",
    "| **Sensor Relationships** | Model physical/functional connections between sensors |\n",
    "| **Distinguish Faults** | Separate sensor faults from system faults |\n",
    "| **Scalable** | Works with varying number of sensors/machines |\n",
    "| **Explainable** | Attention weights show which connections matter |\n",
    "\n",
    "## Graph Representation\n",
    "\n",
    "```\n",
    "Nodes = Sensors or Components\n",
    "Edges = Physical connections, correlations, or proximity\n",
    "Node Features = Sensor readings over time\n",
    "Edge Features = Distance, connection type, correlation strength\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score\n",
    "import seaborn as sns\n",
    "import os\n",
    "import json\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "# Check TensorFlow availability\n",
    "try:\n",
    "    import tensorflow as tf\n",
    "    from tensorflow import keras\n",
    "    from tensorflow.keras import layers\n",
    "    print(f\"TensorFlow {tf.__version__} available\")\n",
    "    HAS_TF = True\n",
    "except ImportError:\n",
    "    print(\"TensorFlow not available\")\n",
    "    HAS_TF = False\n",
    "\n",
    "# Output directories\n",
    "DATA_DIR = '../data/gnn'\n",
    "MODEL_DIR = '../models/gnn'\n",
    "os.makedirs(DATA_DIR, exist_ok=True)\n",
    "os.makedirs(MODEL_DIR, exist_ok=True)\n",
    "\n",
    "print(\"Setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. GNN Building Blocks\n",
    "\n",
    "We implement GNN layers from scratch using TensorFlow for maximum compatibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if HAS_TF:\n",
    "    \n",
    "    class GraphConvolution(layers.Layer):\n",
    "        \"\"\"\n",
    "        Graph Convolutional Layer (GCN).\n",
    "        \n",
    "        Aggregates features from neighboring nodes:\n",
    "        h_v = σ(W · MEAN(h_u for u in N(v) ∪ {v}))\n",
    "        \"\"\"\n",
    "        \n",
    "        def __init__(self, units, activation='relu', use_bias=True, **kwargs):\n",
    "            super().__init__(**kwargs)\n",
    "            self.units = units\n",
    "            self.activation = keras.activations.get(activation)\n",
    "            self.use_bias = use_bias\n",
    "            \n",
    "        def build(self, input_shape):\n",
    "            # input_shape: (node_features_shape, adjacency_shape)\n",
    "            feature_dim = input_shape[0][-1]\n",
    "            \n",
    "            self.kernel = self.add_weight(\n",
    "                name='kernel',\n",
    "                shape=(feature_dim, self.units),\n",
    "                initializer='glorot_uniform',\n",
    "                trainable=True\n",
    "            )\n",
    "            \n",
    "            if self.use_bias:\n",
    "                self.bias = self.add_weight(\n",
    "                    name='bias',\n",
    "                    shape=(self.units,),\n",
    "                    initializer='zeros',\n",
    "                    trainable=True\n",
    "                )\n",
    "            \n",
    "            super().build(input_shape)\n",
    "            \n",
    "        def call(self, inputs):\n",
    "            \"\"\"\n",
    "            Args:\n",
    "                inputs: tuple of (node_features, adjacency_matrix)\n",
    "                    node_features: [batch, n_nodes, features]\n",
    "                    adjacency: [batch, n_nodes, n_nodes] or [n_nodes, n_nodes]\n",
    "                    \n",
    "            Returns:\n",
    "                Updated node features: [batch, n_nodes, units]\n",
    "            \"\"\"\n",
    "            node_features, adjacency = inputs\n",
    "            \n",
    "            # Normalize adjacency (add self-loops and normalize)\n",
    "            adj = adjacency\n",
    "            if len(adj.shape) == 2:\n",
    "                # Shared adjacency for all samples\n",
    "                adj = tf.expand_dims(adj, 0)  # [1, n, n]\n",
    "            \n",
    "            # Add self-loops\n",
    "            n_nodes = tf.shape(adj)[-1]\n",
    "            identity = tf.eye(n_nodes, dtype=adj.dtype)\n",
    "            adj = adj + identity\n",
    "            \n",
    "            # Degree normalization D^(-1/2) A D^(-1/2)\n",
    "            degree = tf.reduce_sum(adj, axis=-1, keepdims=True)  # [batch, n, 1]\n",
    "            degree_inv_sqrt = tf.math.rsqrt(tf.maximum(degree, 1e-6))\n",
    "            adj_normalized = adj * degree_inv_sqrt * tf.transpose(degree_inv_sqrt, [0, 2, 1])\n",
    "            \n",
    "            # Message passing: aggregate neighbor features\n",
    "            # [batch, n, n] @ [batch, n, features] = [batch, n, features]\n",
    "            aggregated = tf.matmul(adj_normalized, node_features)\n",
    "            \n",
    "            # Transform\n",
    "            output = tf.matmul(aggregated, self.kernel)\n",
    "            \n",
    "            if self.use_bias:\n",
    "                output = output + self.bias\n",
    "                \n",
    "            return self.activation(output)\n",
    "        \n",
    "        def get_config(self):\n",
    "            config = super().get_config()\n",
    "            config.update({'units': self.units})\n",
    "            return config\n",
    "    \n",
    "    print(\"GraphConvolution layer defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if HAS_TF:\n",
    "    \n",
    "    class GraphAttention(layers.Layer):\n",
    "        \"\"\"\n",
    "        Graph Attention Layer (GAT).\n",
    "        \n",
    "        Uses attention mechanism to weight neighbor contributions:\n",
    "        α_ij = softmax(LeakyReLU(a · [Wh_i || Wh_j]))\n",
    "        h'_i = σ(Σ α_ij · W·h_j)\n",
    "        \"\"\"\n",
    "        \n",
    "        def __init__(self, units, n_heads=4, dropout=0.1, **kwargs):\n",
    "            super().__init__(**kwargs)\n",
    "            self.units = units\n",
    "            self.n_heads = n_heads\n",
    "            self.dropout = dropout\n",
    "            self.head_dim = units // n_heads\n",
    "            \n",
    "        def build(self, input_shape):\n",
    "            feature_dim = input_shape[0][-1]\n",
    "            \n",
    "            # Linear transformation for each head\n",
    "            self.W = self.add_weight(\n",
    "                name='W',\n",
    "                shape=(self.n_heads, feature_dim, self.head_dim),\n",
    "                initializer='glorot_uniform',\n",
    "                trainable=True\n",
    "            )\n",
    "            \n",
    "            # Attention weights\n",
    "            self.a_src = self.add_weight(\n",
    "                name='a_src',\n",
    "                shape=(self.n_heads, self.head_dim, 1),\n",
    "                initializer='glorot_uniform',\n",
    "                trainable=True\n",
    "            )\n",
    "            \n",
    "            self.a_dst = self.add_weight(\n",
    "                name='a_dst',\n",
    "                shape=(self.n_heads, self.head_dim, 1),\n",
    "                initializer='glorot_uniform',\n",
    "                trainable=True\n",
    "            )\n",
    "            \n",
    "            self.dropout_layer = layers.Dropout(self.dropout)\n",
    "            \n",
    "            super().build(input_shape)\n",
    "            \n",
    "        def call(self, inputs, training=None):\n",
    "            \"\"\"\n",
    "            Args:\n",
    "                inputs: tuple of (node_features, adjacency_matrix)\n",
    "                    \n",
    "            Returns:\n",
    "                Updated node features with attention\n",
    "            \"\"\"\n",
    "            node_features, adjacency = inputs\n",
    "            batch_size = tf.shape(node_features)[0]\n",
    "            n_nodes = tf.shape(node_features)[1]\n",
    "            \n",
    "            # Transform features for each head\n",
    "            # [batch, n_nodes, features] -> [batch, n_heads, n_nodes, head_dim]\n",
    "            h = tf.einsum('bni,hio->bhno', node_features, self.W)\n",
    "            \n",
    "            # Compute attention scores\n",
    "            # Source attention: [batch, n_heads, n_nodes, 1]\n",
    "            attn_src = tf.einsum('bhni,hio->bhno', h, self.a_src)\n",
    "            # Target attention: [batch, n_heads, n_nodes, 1]\n",
    "            attn_dst = tf.einsum('bhni,hio->bhno', h, self.a_dst)\n",
    "            \n",
    "            # Pairwise attention: e_ij = attn_src_i + attn_dst_j\n",
    "            # [batch, n_heads, n_nodes, 1] + [batch, n_heads, 1, n_nodes]\n",
    "            attn = attn_src + tf.transpose(attn_dst, [0, 1, 3, 2])\n",
    "            attn = tf.nn.leaky_relu(attn, alpha=0.2)\n",
    "            \n",
    "            # Mask with adjacency (only attend to neighbors)\n",
    "            if len(adjacency.shape) == 2:\n",
    "                adjacency = tf.expand_dims(adjacency, 0)\n",
    "            mask = tf.expand_dims(adjacency, 1)  # [batch, 1, n, n]\n",
    "            attn = tf.where(mask > 0, attn, tf.ones_like(attn) * -1e9)\n",
    "            \n",
    "            # Softmax attention\n",
    "            attn = tf.nn.softmax(attn, axis=-1)\n",
    "            attn = self.dropout_layer(attn, training=training)\n",
    "            \n",
    "            # Aggregate with attention weights\n",
    "            # [batch, n_heads, n_nodes, n_nodes] @ [batch, n_heads, n_nodes, head_dim]\n",
    "            output = tf.einsum('bhnm,bhmd->bhnd', attn, h)\n",
    "            \n",
    "            # Concatenate heads\n",
    "            output = tf.transpose(output, [0, 2, 1, 3])  # [batch, n_nodes, n_heads, head_dim]\n",
    "            output = tf.reshape(output, [batch_size, n_nodes, self.units])\n",
    "            \n",
    "            return output\n",
    "        \n",
    "        def get_config(self):\n",
    "            config = super().get_config()\n",
    "            config.update({\n",
    "                'units': self.units,\n",
    "                'n_heads': self.n_heads,\n",
    "                'dropout': self.dropout\n",
    "            })\n",
    "            return config\n",
    "    \n",
    "    print(\"GraphAttention (GAT) layer defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if HAS_TF:\n",
    "    \n",
    "    class SpatioTemporalGNN(layers.Layer):\n",
    "        \"\"\"\n",
    "        Spatio-Temporal Graph Neural Network.\n",
    "        \n",
    "        Combines:\n",
    "        - Spatial: Graph convolution across sensors\n",
    "        - Temporal: 1D convolution along time\n",
    "        \"\"\"\n",
    "        \n",
    "        def __init__(self, spatial_units, temporal_units, kernel_size=3, **kwargs):\n",
    "            super().__init__(**kwargs)\n",
    "            self.spatial_units = spatial_units\n",
    "            self.temporal_units = temporal_units\n",
    "            self.kernel_size = kernel_size\n",
    "            \n",
    "        def build(self, input_shape):\n",
    "            # Spatial: Graph convolution\n",
    "            self.graph_conv = GraphConvolution(self.spatial_units)\n",
    "            \n",
    "            # Temporal: 1D convolution\n",
    "            self.temporal_conv = layers.Conv1D(\n",
    "                self.temporal_units,\n",
    "                kernel_size=self.kernel_size,\n",
    "                padding='same',\n",
    "                activation='relu'\n",
    "            )\n",
    "            \n",
    "            self.norm = layers.LayerNormalization()\n",
    "            \n",
    "            super().build(input_shape)\n",
    "            \n",
    "        def call(self, inputs):\n",
    "            \"\"\"\n",
    "            Args:\n",
    "                inputs: tuple of (node_features, adjacency)\n",
    "                    node_features: [batch, n_nodes, time_steps, features]\n",
    "                    adjacency: [n_nodes, n_nodes]\n",
    "            \"\"\"\n",
    "            node_features, adjacency = inputs\n",
    "            batch_size = tf.shape(node_features)[0]\n",
    "            n_nodes = tf.shape(node_features)[1]\n",
    "            time_steps = tf.shape(node_features)[2]\n",
    "            features = tf.shape(node_features)[3]\n",
    "            \n",
    "            # Reshape for temporal processing: [batch * n_nodes, time, features]\n",
    "            x = tf.reshape(node_features, [-1, time_steps, features])\n",
    "            x = self.temporal_conv(x)\n",
    "            \n",
    "            # Reshape back: [batch, n_nodes, time, temporal_units]\n",
    "            x = tf.reshape(x, [batch_size, n_nodes, time_steps, self.temporal_units])\n",
    "            \n",
    "            # Spatial processing at each time step\n",
    "            # [batch, n_nodes, time, units] -> process each time step\n",
    "            outputs = []\n",
    "            for t in range(time_steps):\n",
    "                x_t = x[:, :, t, :]  # [batch, n_nodes, units]\n",
    "                x_t = self.graph_conv([x_t, adjacency])\n",
    "                outputs.append(x_t)\n",
    "            \n",
    "            # Stack: [batch, n_nodes, time, spatial_units]\n",
    "            output = tf.stack(outputs, axis=2)\n",
    "            output = self.norm(output)\n",
    "            \n",
    "            return output\n",
    "        \n",
    "        def get_config(self):\n",
    "            config = super().get_config()\n",
    "            config.update({\n",
    "                'spatial_units': self.spatial_units,\n",
    "                'temporal_units': self.temporal_units\n",
    "            })\n",
    "            return config\n",
    "    \n",
    "    print(\"SpatioTemporalGNN layer defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Create Sensor Network Data\n",
    "\n",
    "Simulate a multi-sensor system with known topology."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sensor_network(n_sensors=12):\n",
    "    \"\"\"\n",
    "    Create a sensor network topology representing a pump system.\n",
    "    \n",
    "    Sensors:\n",
    "    0: Motor vibration\n",
    "    1: Motor temperature\n",
    "    2: Motor current\n",
    "    3: Coupling vibration\n",
    "    4: Pump inlet pressure\n",
    "    5: Pump outlet pressure\n",
    "    6: Pump vibration (bearing 1)\n",
    "    7: Pump vibration (bearing 2)\n",
    "    8: Pump temperature\n",
    "    9: Flow rate\n",
    "    10: Discharge temperature\n",
    "    11: Control valve position\n",
    "    \"\"\"\n",
    "    sensor_names = [\n",
    "        'motor_vib', 'motor_temp', 'motor_current', 'coupling_vib',\n",
    "        'inlet_pressure', 'outlet_pressure', 'pump_vib_1', 'pump_vib_2',\n",
    "        'pump_temp', 'flow_rate', 'discharge_temp', 'valve_pos'\n",
    "    ]\n",
    "    \n",
    "    # Adjacency matrix (physical connections)\n",
    "    adj = np.zeros((n_sensors, n_sensors))\n",
    "    \n",
    "    # Motor group (0, 1, 2)\n",
    "    adj[0, 1] = adj[1, 0] = 1  # vib-temp\n",
    "    adj[0, 2] = adj[2, 0] = 1  # vib-current\n",
    "    adj[1, 2] = adj[2, 1] = 1  # temp-current\n",
    "    \n",
    "    # Motor to coupling\n",
    "    adj[0, 3] = adj[3, 0] = 1\n",
    "    \n",
    "    # Coupling to pump bearings\n",
    "    adj[3, 6] = adj[6, 3] = 1\n",
    "    adj[3, 7] = adj[7, 3] = 1\n",
    "    \n",
    "    # Pump bearing group (6, 7, 8)\n",
    "    adj[6, 7] = adj[7, 6] = 1\n",
    "    adj[6, 8] = adj[8, 6] = 1\n",
    "    adj[7, 8] = adj[8, 7] = 1\n",
    "    \n",
    "    # Pressure flow connections\n",
    "    adj[4, 5] = adj[5, 4] = 1  # inlet-outlet\n",
    "    adj[5, 9] = adj[9, 5] = 1  # outlet-flow\n",
    "    adj[9, 11] = adj[11, 9] = 1  # flow-valve\n",
    "    \n",
    "    # Temperature propagation\n",
    "    adj[8, 10] = adj[10, 8] = 1  # pump_temp-discharge\n",
    "    \n",
    "    # Pump to pressure\n",
    "    adj[6, 4] = adj[4, 6] = 1\n",
    "    adj[7, 5] = adj[5, 7] = 1\n",
    "    \n",
    "    return adj.astype(np.float32), sensor_names\n",
    "\n",
    "# Create network\n",
    "adj_matrix, sensor_names = create_sensor_network(n_sensors=12)\n",
    "\n",
    "# Visualize network\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "sns.heatmap(adj_matrix, annot=True, fmt='.0f', cmap='Blues',\n",
    "            xticklabels=sensor_names, yticklabels=sensor_names, ax=ax)\n",
    "plt.title('Sensor Network Adjacency Matrix')\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{DATA_DIR}/sensor_network.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nSensor network created: {len(sensor_names)} nodes, {int(adj_matrix.sum()/2)} edges\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sensor_data_with_faults(\n",
    "    n_samples=500,\n",
    "    n_sensors=12,\n",
    "    time_steps=100,\n",
    "    adj_matrix=None\n",
    "):\n",
    "    \"\"\"\n",
    "    Generate multi-sensor time series data with various fault conditions.\n",
    "    \n",
    "    Faults propagate through the network according to adjacency.\n",
    "    \"\"\"\n",
    "    X = []  # [n_samples, n_sensors, time_steps, features]\n",
    "    y = []  # Fault labels\n",
    "    \n",
    "    fault_types = ['normal', 'motor_fault', 'pump_bearing', 'cavitation', 'valve_stuck']\n",
    "    \n",
    "    for _ in range(n_samples):\n",
    "        fault = np.random.choice(fault_types)\n",
    "        \n",
    "        # Initialize all sensors with baseline behavior\n",
    "        t = np.linspace(0, 2*np.pi, time_steps)\n",
    "        sensor_data = np.zeros((n_sensors, time_steps, 4))  # 4 features per sensor\n",
    "        \n",
    "        # Base signals for each sensor\n",
    "        for s in range(n_sensors):\n",
    "            # Feature 0: Main signal\n",
    "            sensor_data[s, :, 0] = np.sin(t + s * 0.5) + np.random.normal(0, 0.1, time_steps)\n",
    "            # Feature 1: Trend\n",
    "            sensor_data[s, :, 1] = np.random.normal(0, 0.05, time_steps).cumsum() * 0.01\n",
    "            # Feature 2: High frequency\n",
    "            sensor_data[s, :, 2] = np.sin(10 * t) * 0.2 + np.random.normal(0, 0.05, time_steps)\n",
    "            # Feature 3: Level\n",
    "            sensor_data[s, :, 3] = 0.5 + np.random.normal(0, 0.02, time_steps)\n",
    "        \n",
    "        # Apply fault-specific patterns\n",
    "        if fault == 'motor_fault':\n",
    "            # Affects motor sensors (0, 1, 2) and propagates to coupling (3)\n",
    "            sensor_data[0, :, 0] += 1.5 * np.sin(2 * t)  # Increased vibration\n",
    "            sensor_data[1, :, 3] += 0.5 * np.linspace(0, 1, time_steps)  # Rising temp\n",
    "            sensor_data[2, :, 0] += 0.8 * np.abs(np.sin(t))  # Current spikes\n",
    "            sensor_data[3, :, 0] += 0.7 * np.sin(2 * t)  # Coupling vibration\n",
    "            \n",
    "        elif fault == 'pump_bearing':\n",
    "            # Affects pump bearings (6, 7) and temperature (8)\n",
    "            # Add high frequency impulses\n",
    "            impulses = np.zeros(time_steps)\n",
    "            impulse_locs = np.linspace(10, time_steps-10, 8).astype(int)\n",
    "            impulses[impulse_locs] = np.random.uniform(1, 2, 8)\n",
    "            sensor_data[6, :, 0] += impulses\n",
    "            sensor_data[6, :, 2] += 0.8  # High frequency energy\n",
    "            sensor_data[7, :, 0] += impulses * 0.8\n",
    "            sensor_data[7, :, 2] += 0.6\n",
    "            sensor_data[8, :, 3] += 0.3 * np.linspace(0, 1, time_steps)  # Temp rise\n",
    "            \n",
    "        elif fault == 'cavitation':\n",
    "            # Affects inlet pressure (4), pump vibrations (6, 7), noise increase\n",
    "            sensor_data[4, :, 0] -= 0.5  # Low inlet pressure\n",
    "            sensor_data[4, :, 3] -= 0.3\n",
    "            # Broadband noise on pump\n",
    "            sensor_data[6, :, :] += np.random.normal(0, 0.5, (time_steps, 4))\n",
    "            sensor_data[7, :, :] += np.random.normal(0, 0.4, (time_steps, 4))\n",
    "            sensor_data[9, :, 0] -= 0.3  # Reduced flow (unstable)\n",
    "            \n",
    "        elif fault == 'valve_stuck':\n",
    "            # Valve (11) doesn't respond, affects flow (9) and pressure (5)\n",
    "            sensor_data[11, :, 0] = 0.7 + np.random.normal(0, 0.01, time_steps)  # Stuck\n",
    "            sensor_data[11, :, 1] = 0  # No trend\n",
    "            sensor_data[9, :, 0] *= 0.6  # Reduced flow\n",
    "            sensor_data[5, :, 0] += 0.5  # Higher pressure\n",
    "            sensor_data[5, :, 3] += 0.3\n",
    "        \n",
    "        X.append(sensor_data)\n",
    "        y.append(fault)\n",
    "    \n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# Generate data\n",
    "print(\"Generating sensor network data with faults...\")\n",
    "X_graph, y_graph = generate_sensor_data_with_faults(\n",
    "    n_samples=1000,\n",
    "    n_sensors=12,\n",
    "    time_steps=100,\n",
    "    adj_matrix=adj_matrix\n",
    ")\n",
    "\n",
    "print(f\"Generated: X={X_graph.shape}\")\n",
    "print(f\"  Shape: [samples, sensors, time_steps, features]\")\n",
    "print(f\"Classes: {np.unique(y_graph, return_counts=True)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize different fault patterns\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 8))\n",
    "fault_types = ['normal', 'motor_fault', 'pump_bearing', 'cavitation', 'valve_stuck']\n",
    "\n",
    "for idx, fault in enumerate(fault_types):\n",
    "    ax = axes.flat[idx]\n",
    "    # Find example of this fault\n",
    "    sample_idx = np.where(y_graph == fault)[0][0]\n",
    "    \n",
    "    # Plot several sensors\n",
    "    for s in [0, 4, 6, 9]:  # motor_vib, inlet_p, pump_vib, flow\n",
    "        ax.plot(X_graph[sample_idx, s, :, 0], label=sensor_names[s], alpha=0.7)\n",
    "    \n",
    "    ax.set_title(f'Fault: {fault}')\n",
    "    ax.set_xlabel('Time')\n",
    "    ax.set_ylabel('Signal')\n",
    "    if idx == 0:\n",
    "        ax.legend(fontsize=8)\n",
    "\n",
    "axes.flat[-1].axis('off')  # Hide last empty subplot\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{DATA_DIR}/fault_patterns.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Build GNN Fault Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if HAS_TF:\n",
    "    \n",
    "    def build_gnn_classifier(\n",
    "        n_sensors,\n",
    "        time_steps,\n",
    "        n_features,\n",
    "        n_classes,\n",
    "        gcn_units=[64, 32],\n",
    "        use_attention=True\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Build a GNN-based fault classifier.\n",
    "        \n",
    "        Architecture:\n",
    "        1. Temporal processing per sensor (1D Conv)\n",
    "        2. Graph convolution across sensors\n",
    "        3. Global pooling and classification\n",
    "        \"\"\"\n",
    "        # Inputs\n",
    "        node_input = keras.Input(shape=(n_sensors, time_steps, n_features), name='node_features')\n",
    "        adj_input = keras.Input(shape=(n_sensors, n_sensors), name='adjacency')\n",
    "        \n",
    "        # Step 1: Temporal processing per sensor\n",
    "        # Reshape: [batch, n_sensors, time, features] -> [batch*n_sensors, time, features]\n",
    "        batch_size = tf.shape(node_input)[0]\n",
    "        x = tf.reshape(node_input, [-1, time_steps, n_features])\n",
    "        \n",
    "        # 1D convolutions\n",
    "        x = layers.Conv1D(32, 5, padding='same', activation='relu')(x)\n",
    "        x = layers.Conv1D(64, 3, padding='same', activation='relu')(x)\n",
    "        x = layers.GlobalAveragePooling1D()(x)  # [batch*n_sensors, 64]\n",
    "        \n",
    "        # Reshape back: [batch, n_sensors, 64]\n",
    "        x = tf.reshape(x, [batch_size, n_sensors, 64])\n",
    "        \n",
    "        # Step 2: Graph convolutions\n",
    "        if use_attention:\n",
    "            x = GraphAttention(gcn_units[0], n_heads=4)([x, adj_input])\n",
    "            x = layers.ReLU()(x)\n",
    "            x = layers.Dropout(0.2)(x)\n",
    "            x = GraphAttention(gcn_units[1], n_heads=2)([x, adj_input])\n",
    "        else:\n",
    "            for units in gcn_units:\n",
    "                x = GraphConvolution(units)([x, adj_input])\n",
    "                x = layers.Dropout(0.2)(x)\n",
    "        \n",
    "        # Step 3: Global pooling\n",
    "        x = layers.GlobalAveragePooling1D()(x)  # Aggregate across nodes\n",
    "        \n",
    "        # Classification head\n",
    "        x = layers.Dense(64, activation='relu')(x)\n",
    "        x = layers.Dropout(0.3)(x)\n",
    "        outputs = layers.Dense(n_classes, activation='softmax')(x)\n",
    "        \n",
    "        model = keras.Model(\n",
    "            inputs=[node_input, adj_input],\n",
    "            outputs=outputs\n",
    "        )\n",
    "        return model\n",
    "    \n",
    "    # Prepare data\n",
    "    le = LabelEncoder()\n",
    "    y_encoded = le.fit_transform(y_graph)\n",
    "    n_classes = len(le.classes_)\n",
    "    \n",
    "    # Normalize per sensor\n",
    "    X_normalized = np.zeros_like(X_graph)\n",
    "    for i in range(X_graph.shape[0]):\n",
    "        for s in range(X_graph.shape[1]):\n",
    "            scaler = StandardScaler()\n",
    "            X_normalized[i, s] = scaler.fit_transform(X_graph[i, s])\n",
    "    \n",
    "    # Split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X_normalized, y_encoded, test_size=0.2, random_state=42, stratify=y_encoded\n",
    "    )\n",
    "    \n",
    "    # Adjacency for all samples (same structure)\n",
    "    adj_train = np.tile(adj_matrix[np.newaxis, :, :], (len(X_train), 1, 1))\n",
    "    adj_test = np.tile(adj_matrix[np.newaxis, :, :], (len(X_test), 1, 1))\n",
    "    \n",
    "    print(f\"Training: {X_train.shape}\")\n",
    "    print(f\"Test: {X_test.shape}\")\n",
    "    print(f\"Classes: {le.classes_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if HAS_TF:\n",
    "    # Build model\n",
    "    gnn_model = build_gnn_classifier(\n",
    "        n_sensors=12,\n",
    "        time_steps=100,\n",
    "        n_features=4,\n",
    "        n_classes=n_classes,\n",
    "        gcn_units=[64, 32],\n",
    "        use_attention=True\n",
    "    )\n",
    "    \n",
    "    gnn_model.compile(\n",
    "        optimizer=keras.optimizers.Adam(1e-3),\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    gnn_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if HAS_TF:\n",
    "    print(\"Training GNN Classifier...\")\n",
    "    history = gnn_model.fit(\n",
    "        [X_train, adj_train], y_train,\n",
    "        validation_split=0.15,\n",
    "        epochs=50,\n",
    "        batch_size=32,\n",
    "        callbacks=[\n",
    "            keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True),\n",
    "            keras.callbacks.ReduceLROnPlateau(factor=0.5, patience=5)\n",
    "        ],\n",
    "        verbose=1\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if HAS_TF:\n",
    "    # Evaluate\n",
    "    y_pred = gnn_model.predict([X_test, adj_test]).argmax(axis=1)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"GNN Classifier Results:\")\n",
    "    print(\"=\"*50)\n",
    "    print(classification_report(y_test, y_pred, target_names=le.classes_))\n",
    "    \n",
    "    # Confusion matrix\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # Training curves\n",
    "    axes[0].plot(history.history['accuracy'], label='Train')\n",
    "    axes[0].plot(history.history['val_accuracy'], label='Val')\n",
    "    axes[0].set_xlabel('Epoch')\n",
    "    axes[0].set_ylabel('Accuracy')\n",
    "    axes[0].set_title('GNN Training History')\n",
    "    axes[0].legend()\n",
    "    \n",
    "    # Confusion matrix\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "                xticklabels=le.classes_, yticklabels=le.classes_, ax=axes[1])\n",
    "    axes[1].set_xlabel('Predicted')\n",
    "    axes[1].set_ylabel('Actual')\n",
    "    axes[1].set_title('Confusion Matrix')\n",
    "    plt.xticks(rotation=45)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{MODEL_DIR}/gnn_results.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. GNN for Anomaly Detection\n",
    "\n",
    "Detect when sensor readings deviate from expected network-wide patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if HAS_TF:\n",
    "    \n",
    "    def build_gnn_autoencoder(n_sensors, time_steps, n_features, latent_dim=16):\n",
    "        \"\"\"\n",
    "        Graph Autoencoder for anomaly detection.\n",
    "        \n",
    "        Learns to reconstruct normal sensor patterns.\n",
    "        Anomalies have high reconstruction error.\n",
    "        \"\"\"\n",
    "        # Inputs\n",
    "        node_input = keras.Input(shape=(n_sensors, time_steps, n_features))\n",
    "        adj_input = keras.Input(shape=(n_sensors, n_sensors))\n",
    "        \n",
    "        batch_size = tf.shape(node_input)[0]\n",
    "        \n",
    "        # Encoder: Temporal then Graph\n",
    "        x = tf.reshape(node_input, [-1, time_steps, n_features])\n",
    "        x = layers.Conv1D(32, 5, padding='same', activation='relu')(x)\n",
    "        x = layers.Conv1D(16, 3, padding='same', activation='relu')(x)\n",
    "        x = layers.GlobalAveragePooling1D()(x)\n",
    "        x = tf.reshape(x, [batch_size, n_sensors, 16])\n",
    "        \n",
    "        # Graph convolution to capture network patterns\n",
    "        x = GraphConvolution(32)([x, adj_input])\n",
    "        x = GraphConvolution(latent_dim)([x, adj_input])\n",
    "        \n",
    "        # Latent representation\n",
    "        latent = layers.GlobalAveragePooling1D()(x)  # [batch, latent_dim]\n",
    "        \n",
    "        # Decoder: Expand back to sensors\n",
    "        x = layers.Dense(n_sensors * latent_dim)(latent)\n",
    "        x = tf.reshape(x, [batch_size, n_sensors, latent_dim])\n",
    "        \n",
    "        x = GraphConvolution(32)([x, adj_input])\n",
    "        x = GraphConvolution(16)([x, adj_input])\n",
    "        \n",
    "        # Expand to time series\n",
    "        x = layers.Dense(time_steps * n_features)(x)\n",
    "        outputs = tf.reshape(x, [batch_size, n_sensors, time_steps, n_features])\n",
    "        \n",
    "        model = keras.Model(\n",
    "            inputs=[node_input, adj_input],\n",
    "            outputs=outputs\n",
    "        )\n",
    "        return model\n",
    "    \n",
    "    # Train on normal data only\n",
    "    normal_mask = y_graph == 'normal'\n",
    "    X_normal = X_normalized[normal_mask]\n",
    "    \n",
    "    X_train_ae, X_val_ae = train_test_split(X_normal, test_size=0.2, random_state=42)\n",
    "    adj_train_ae = np.tile(adj_matrix[np.newaxis, :, :], (len(X_train_ae), 1, 1))\n",
    "    adj_val_ae = np.tile(adj_matrix[np.newaxis, :, :], (len(X_val_ae), 1, 1))\n",
    "    \n",
    "    # Build model\n",
    "    gnn_ae = build_gnn_autoencoder(\n",
    "        n_sensors=12,\n",
    "        time_steps=100,\n",
    "        n_features=4,\n",
    "        latent_dim=8\n",
    "    )\n",
    "    \n",
    "    gnn_ae.compile(\n",
    "        optimizer=keras.optimizers.Adam(1e-3),\n",
    "        loss='mse'\n",
    "    )\n",
    "    \n",
    "    print(f\"Training GNN Autoencoder on {len(X_train_ae)} normal samples...\")\n",
    "    gnn_ae.fit(\n",
    "        [X_train_ae, adj_train_ae], X_train_ae,\n",
    "        validation_data=([X_val_ae, adj_val_ae], X_val_ae),\n",
    "        epochs=30,\n",
    "        batch_size=32,\n",
    "        callbacks=[\n",
    "            keras.callbacks.EarlyStopping(patience=5, restore_best_weights=True)\n",
    "        ],\n",
    "        verbose=1\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if HAS_TF:\n",
    "    # Compute anomaly scores\n",
    "    adj_all = np.tile(adj_matrix[np.newaxis, :, :], (len(X_normalized), 1, 1))\n",
    "    \n",
    "    reconstructions = gnn_ae.predict([X_normalized, adj_all], verbose=0)\n",
    "    \n",
    "    # Reconstruction error per sample\n",
    "    errors = np.mean((X_normalized - reconstructions) ** 2, axis=(1, 2, 3))\n",
    "    \n",
    "    # Separate by class\n",
    "    error_by_class = {}\n",
    "    for fault in np.unique(y_graph):\n",
    "        mask = y_graph == fault\n",
    "        error_by_class[fault] = errors[mask]\n",
    "    \n",
    "    # Visualize\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # Box plot by class\n",
    "    data_to_plot = [error_by_class[f] for f in np.unique(y_graph)]\n",
    "    bp = axes[0].boxplot(data_to_plot, labels=np.unique(y_graph), patch_artist=True)\n",
    "    colors = plt.cm.Set3(np.linspace(0, 1, len(data_to_plot)))\n",
    "    for patch, color in zip(bp['boxes'], colors):\n",
    "        patch.set_facecolor(color)\n",
    "    axes[0].set_ylabel('Reconstruction Error')\n",
    "    axes[0].set_title('GNN Autoencoder: Error by Fault Type')\n",
    "    axes[0].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # ROC curve (normal vs all faults)\n",
    "    labels_binary = (y_graph != 'normal').astype(int)\n",
    "    \n",
    "    from sklearn.metrics import roc_curve, auc\n",
    "    fpr, tpr, _ = roc_curve(labels_binary, errors)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    \n",
    "    axes[1].plot(fpr, tpr, 'b-', linewidth=2, label=f'AUC = {roc_auc:.3f}')\n",
    "    axes[1].plot([0, 1], [0, 1], 'k--', alpha=0.3)\n",
    "    axes[1].set_xlabel('False Positive Rate')\n",
    "    axes[1].set_ylabel('True Positive Rate')\n",
    "    axes[1].set_title('GNN Autoencoder: ROC Curve')\n",
    "    axes[1].legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{MODEL_DIR}/gnn_anomaly_detection.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"\\nGNN Autoencoder AUC: {roc_auc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Multi-Machine Factory Monitoring\n",
    "\n",
    "Model relationships between multiple machines in a factory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_factory_graph(n_machines=8):\n",
    "    \"\"\"\n",
    "    Create a factory floor layout with machine dependencies.\n",
    "    \n",
    "    Layout:\n",
    "    [M0] -> [M1] -> [M2] -> [M3]  (Production Line 1)\n",
    "             |       |\n",
    "    [M4] -> [M5] -> [M6] -> [M7]  (Production Line 2)\n",
    "    \n",
    "    Shared utilities, proximity effects, etc.\n",
    "    \"\"\"\n",
    "    adj = np.zeros((n_machines, n_machines))\n",
    "    \n",
    "    # Line 1 flow\n",
    "    adj[0, 1] = adj[1, 0] = 1\n",
    "    adj[1, 2] = adj[2, 1] = 1\n",
    "    adj[2, 3] = adj[3, 2] = 1\n",
    "    \n",
    "    # Line 2 flow\n",
    "    adj[4, 5] = adj[5, 4] = 1\n",
    "    adj[5, 6] = adj[6, 5] = 1\n",
    "    adj[6, 7] = adj[7, 6] = 1\n",
    "    \n",
    "    # Cross-line dependencies (shared resources)\n",
    "    adj[1, 5] = adj[5, 1] = 1  # Shared utility\n",
    "    adj[2, 6] = adj[6, 2] = 1  # Proximity\n",
    "    \n",
    "    machine_names = [f'Machine_{i}' for i in range(n_machines)]\n",
    "    \n",
    "    return adj.astype(np.float32), machine_names\n",
    "\n",
    "factory_adj, machine_names = create_factory_graph(8)\n",
    "\n",
    "# Visualize\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(factory_adj, annot=True, fmt='.0f', cmap='Greens',\n",
    "            xticklabels=machine_names, yticklabels=machine_names)\n",
    "plt.title('Factory Machine Network')\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{DATA_DIR}/factory_network.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_factory_health_data(n_samples=500, n_machines=8, time_steps=50):\n",
    "    \"\"\"\n",
    "    Generate factory-level health monitoring data.\n",
    "    \n",
    "    Features per machine: [health_index, production_rate, energy, alerts]\n",
    "    \"\"\"\n",
    "    X = []\n",
    "    y = []  # Overall factory status\n",
    "    \n",
    "    statuses = ['healthy', 'degrading', 'bottleneck', 'failure_propagating']\n",
    "    \n",
    "    for _ in range(n_samples):\n",
    "        status = np.random.choice(statuses, p=[0.5, 0.2, 0.15, 0.15])\n",
    "        \n",
    "        machine_data = np.zeros((n_machines, time_steps, 4))\n",
    "        \n",
    "        for m in range(n_machines):\n",
    "            t = np.linspace(0, 1, time_steps)\n",
    "            \n",
    "            # Base healthy operation\n",
    "            health = 0.9 + np.random.normal(0, 0.02, time_steps)\n",
    "            production = 0.85 + np.random.normal(0, 0.05, time_steps)\n",
    "            energy = 0.7 + np.random.normal(0, 0.03, time_steps)\n",
    "            alerts = np.random.poisson(0.1, time_steps).astype(float)\n",
    "            \n",
    "            if status == 'degrading':\n",
    "                # Random machine degrading\n",
    "                if np.random.random() < 0.3:\n",
    "                    health -= 0.3 * t\n",
    "                    energy += 0.2 * t\n",
    "                    alerts += np.random.poisson(0.5, time_steps)\n",
    "                    \n",
    "            elif status == 'bottleneck':\n",
    "                # Machine 2 or 6 causing bottleneck\n",
    "                if m in [2, 6]:\n",
    "                    production *= 0.5\n",
    "                    health -= 0.1\n",
    "                # Downstream affected\n",
    "                if m in [3, 7]:\n",
    "                    production *= 0.6  # Starved\n",
    "                    \n",
    "            elif status == 'failure_propagating':\n",
    "                # Failure at M1 propagates to M2, M5\n",
    "                if m == 1:\n",
    "                    health = 0.2 + np.random.normal(0, 0.05, time_steps)\n",
    "                    production *= 0.1\n",
    "                    alerts = np.random.poisson(2, time_steps).astype(float)\n",
    "                elif m in [2, 5]:  # Connected to M1\n",
    "                    health -= 0.2\n",
    "                    production *= 0.4\n",
    "                    alerts += np.random.poisson(0.8, time_steps)\n",
    "            \n",
    "            machine_data[m, :, 0] = np.clip(health, 0, 1)\n",
    "            machine_data[m, :, 1] = np.clip(production, 0, 1)\n",
    "            machine_data[m, :, 2] = np.clip(energy, 0, 1)\n",
    "            machine_data[m, :, 3] = alerts\n",
    "        \n",
    "        X.append(machine_data)\n",
    "        y.append(status)\n",
    "    \n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# Generate factory data\n",
    "print(\"Generating factory health data...\")\n",
    "X_factory, y_factory = generate_factory_health_data(n_samples=800)\n",
    "print(f\"Generated: X={X_factory.shape}\")\n",
    "print(f\"Classes: {np.unique(y_factory, return_counts=True)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if HAS_TF:\n",
    "    # Build and train factory-level GNN\n",
    "    le_factory = LabelEncoder()\n",
    "    y_factory_enc = le_factory.fit_transform(y_factory)\n",
    "    \n",
    "    # Split\n",
    "    X_train_f, X_test_f, y_train_f, y_test_f = train_test_split(\n",
    "        X_factory, y_factory_enc, test_size=0.2, random_state=42, stratify=y_factory_enc\n",
    "    )\n",
    "    \n",
    "    adj_train_f = np.tile(factory_adj[np.newaxis, :, :], (len(X_train_f), 1, 1))\n",
    "    adj_test_f = np.tile(factory_adj[np.newaxis, :, :], (len(X_test_f), 1, 1))\n",
    "    \n",
    "    # Build model\n",
    "    factory_gnn = build_gnn_classifier(\n",
    "        n_sensors=8,  # machines\n",
    "        time_steps=50,\n",
    "        n_features=4,\n",
    "        n_classes=len(le_factory.classes_),\n",
    "        gcn_units=[32, 16],\n",
    "        use_attention=True\n",
    "    )\n",
    "    \n",
    "    factory_gnn.compile(\n",
    "        optimizer=keras.optimizers.Adam(1e-3),\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    print(\"Training Factory GNN...\")\n",
    "    factory_gnn.fit(\n",
    "        [X_train_f, adj_train_f], y_train_f,\n",
    "        validation_split=0.15,\n",
    "        epochs=40,\n",
    "        batch_size=32,\n",
    "        callbacks=[\n",
    "            keras.callbacks.EarlyStopping(patience=8, restore_best_weights=True)\n",
    "        ],\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    # Evaluate\n",
    "    y_pred_f = factory_gnn.predict([X_test_f, adj_test_f]).argmax(axis=1)\n",
    "    print(\"\\nFactory GNN Results:\")\n",
    "    print(classification_report(y_test_f, y_pred_f, target_names=le_factory.classes_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Save Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if HAS_TF:\n",
    "    # Save models\n",
    "    gnn_model.save(f'{MODEL_DIR}/gnn_sensor_classifier.keras')\n",
    "    gnn_ae.save(f'{MODEL_DIR}/gnn_autoencoder.keras')\n",
    "    factory_gnn.save(f'{MODEL_DIR}/gnn_factory_monitor.keras')\n",
    "    \n",
    "    # Save adjacency matrices\n",
    "    np.save(f'{MODEL_DIR}/sensor_adjacency.npy', adj_matrix)\n",
    "    np.save(f'{MODEL_DIR}/factory_adjacency.npy', factory_adj)\n",
    "    \n",
    "    # Save metadata\n",
    "    metadata = {\n",
    "        'model_type': 'Graph Neural Network',\n",
    "        'models': {\n",
    "            'sensor_classifier': {\n",
    "                'file': 'gnn_sensor_classifier.keras',\n",
    "                'adjacency': 'sensor_adjacency.npy',\n",
    "                'n_sensors': 12,\n",
    "                'sensor_names': sensor_names,\n",
    "                'classes': le.classes_.tolist()\n",
    "            },\n",
    "            'autoencoder': {\n",
    "                'file': 'gnn_autoencoder.keras',\n",
    "                'auc': float(roc_auc)\n",
    "            },\n",
    "            'factory_monitor': {\n",
    "                'file': 'gnn_factory_monitor.keras',\n",
    "                'adjacency': 'factory_adjacency.npy',\n",
    "                'n_machines': 8,\n",
    "                'classes': le_factory.classes_.tolist()\n",
    "            }\n",
    "        },\n",
    "        'advantages': [\n",
    "            'Models sensor/machine relationships explicitly',\n",
    "            'Distinguishes sensor faults from system faults',\n",
    "            'Scalable to varying network sizes',\n",
    "            'Attention weights provide explainability'\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    with open(f'{MODEL_DIR}/gnn_metadata.json', 'w') as f:\n",
    "        json.dump(metadata, f, indent=2)\n",
    "    \n",
    "    print(f\"\\nModels saved to {MODEL_DIR}/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Node-RED Integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_red_code = '''\n",
    "// Node-RED Function: GNN Sensor Network Monitoring\n",
    "// Collects data from multiple sensors and predicts system health\n",
    "\n",
    "const N_SENSORS = 12;\n",
    "const TIME_WINDOW = 100;\n",
    "\n",
    "// Sensor network topology (adjacency matrix)\n",
    "// This should match the trained model's expected structure\n",
    "const ADJACENCY = flow.get(\"sensor_adjacency\") || [\n",
    "    // Define your sensor connections here\n",
    "    // 1 = connected, 0 = not connected\n",
    "];\n",
    "\n",
    "// Initialize buffers for each sensor\n",
    "if (!context.sensorBuffers) {\n",
    "    context.sensorBuffers = {};\n",
    "    for (let i = 0; i < N_SENSORS; i++) {\n",
    "        context.sensorBuffers[i] = [];\n",
    "    }\n",
    "}\n",
    "\n",
    "// Add reading to appropriate sensor buffer\n",
    "const sensorId = msg.payload.sensorId;\n",
    "const reading = [\n",
    "    msg.payload.value,\n",
    "    msg.payload.trend || 0,\n",
    "    msg.payload.highFreq || 0,\n",
    "    msg.payload.level || 0.5\n",
    "];\n",
    "\n",
    "context.sensorBuffers[sensorId].push(reading);\n",
    "\n",
    "// Keep only last TIME_WINDOW readings\n",
    "if (context.sensorBuffers[sensorId].length > TIME_WINDOW) {\n",
    "    context.sensorBuffers[sensorId].shift();\n",
    "}\n",
    "\n",
    "// Check if all sensors have enough data\n",
    "let ready = true;\n",
    "for (let i = 0; i < N_SENSORS; i++) {\n",
    "    if (context.sensorBuffers[i].length < TIME_WINDOW) {\n",
    "        ready = false;\n",
    "        break;\n",
    "    }\n",
    "}\n",
    "\n",
    "if (!ready) {\n",
    "    msg.payload = { status: \"collecting\", sensors: context.sensorBuffers };\n",
    "    return msg;\n",
    "}\n",
    "\n",
    "// Prepare data for GNN model\n",
    "// Shape: [1, n_sensors, time_steps, features]\n",
    "let nodeFeatures = [];\n",
    "for (let i = 0; i < N_SENSORS; i++) {\n",
    "    nodeFeatures.push(context.sensorBuffers[i]);\n",
    "}\n",
    "\n",
    "msg.payload = {\n",
    "    nodeFeatures: [nodeFeatures],\n",
    "    adjacency: [ADJACENCY],\n",
    "    model: \"gnn_sensor_classifier\"\n",
    "};\n",
    "\n",
    "return msg;\n",
    "''';\n",
    "\n",
    "print(\"Node-RED Integration Code:\")\n",
    "print(\"=\" * 50)\n",
    "print(node_red_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrated **Graph Neural Networks** for Predictive Maintenance:\n",
    "\n",
    "### Key Concepts:\n",
    "\n",
    "| Component | Purpose |\n",
    "|-----------|--------|\n",
    "| **Graph Convolution (GCN)** | Aggregate neighbor features with mean pooling |\n",
    "| **Graph Attention (GAT)** | Weighted aggregation using attention |\n",
    "| **Spatio-Temporal GNN** | Combine graph + temporal patterns |\n",
    "| **Graph Autoencoder** | Anomaly detection via reconstruction |\n",
    "\n",
    "### Use Cases:\n",
    "\n",
    "1. **Sensor Networks** - Model physical connections between sensors\n",
    "2. **Multi-Machine Monitoring** - Factory-level health prediction\n",
    "3. **Fault Propagation** - Track how failures spread through system\n",
    "4. **Sensor vs System Faults** - Distinguish local vs global issues\n",
    "\n",
    "### When to Use GNNs:\n",
    "\n",
    "- Multiple interconnected sensors/machines\n",
    "- Known physical or logical relationships\n",
    "- Need to explain which connections matter\n",
    "- Varying number of nodes (scalable)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
