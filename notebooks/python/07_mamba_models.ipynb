{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mamba (Selective State Space Models) for Predictive Maintenance\n",
    "\n",
    "This notebook implements **Mamba-style State Space Models (SSMs)** for condition monitoring tasks:\n",
    "\n",
    "1. **SSM for RUL Prediction** - Remaining Useful Life estimation\n",
    "2. **SSM for Anomaly Detection** - Autoencoder-based\n",
    "3. **Selective SSM for Classification** - Fault type classification\n",
    "\n",
    "## Why Mamba/SSMs for Predictive Maintenance?\n",
    "\n",
    "| Advantage | Description |\n",
    "|-----------|-------------|\n",
    "| **Linear Complexity** | O(n) vs O(n²) for Transformers - handles very long sequences |\n",
    "| **Input-Selective** | Parameters adapt based on input content |\n",
    "| **Memory Efficient** | Constant memory during inference |\n",
    "| **Long-Range Dependencies** | Captures trends over 10,000+ time steps |\n",
    "\n",
    "## Mamba State Space Equation\n",
    "\n",
    "```\n",
    "h_t = A_t · h_{t-1} + B_t · x_t    (State update - input dependent!)\n",
    "y_t = C_t · h_t + D · x_t           (Output)\n",
    "```\n",
    "\n",
    "The key innovation is that A, B, C are **functions of the input**, making the model selective.\n",
    "\n",
    "## Architecture Overview\n",
    "\n",
    "```\n",
    "Input → Linear Projection → SSM Block(s) → Output Head\n",
    "[T,F]      [T, D]            [T, D]         [1] or [C]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import seaborn as sns\n",
    "import os\n",
    "import json\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "# Check TensorFlow availability\n",
    "try:\n",
    "    import tensorflow as tf\n",
    "    from tensorflow import keras\n",
    "    from tensorflow.keras import layers\n",
    "    print(f\"TensorFlow {tf.__version__} available\")\n",
    "    print(f\"GPU: {tf.config.list_physical_devices('GPU')}\")\n",
    "    HAS_TF = True\n",
    "except ImportError:\n",
    "    print(\"TensorFlow not available - please install: pip install tensorflow\")\n",
    "    HAS_TF = False\n",
    "\n",
    "# Output directories\n",
    "DATA_DIR = '../data/simulated'\n",
    "MODEL_DIR = '../models/mamba'\n",
    "os.makedirs(MODEL_DIR, exist_ok=True)\n",
    "os.makedirs(DATA_DIR, exist_ok=True)\n",
    "\n",
    "print(\"Setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. State Space Model Building Blocks\n",
    "\n",
    "We implement the core SSM components in TensorFlow/Keras.\n",
    "\n",
    "### 1.1 Discretization\n",
    "\n",
    "Continuous SSM parameters are discretized using Zero-Order Hold (ZOH):\n",
    "\n",
    "```\n",
    "Ā = exp(Δ · A)\n",
    "B̄ = (Δ · A)^{-1} · (Ā - I) · Δ · B ≈ Δ · B  (simplified)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if HAS_TF:\n",
    "    \n",
    "    class S4DKernel(layers.Layer):\n",
    "        \"\"\"\n",
    "        Simplified S4D-style SSM kernel.\n",
    "        \n",
    "        Uses diagonal state matrix for efficiency.\n",
    "        \"\"\"\n",
    "        \n",
    "        def __init__(self, d_model, d_state=64, **kwargs):\n",
    "            super().__init__(**kwargs)\n",
    "            self.d_model = d_model\n",
    "            self.d_state = d_state\n",
    "            \n",
    "        def build(self, input_shape):\n",
    "            # Initialize A as diagonal (negative for stability)\n",
    "            # Use HiPPO initialization for better long-range memory\n",
    "            A_real = -0.5 * np.ones(self.d_state)\n",
    "            A_imag = np.pi * np.arange(self.d_state)\n",
    "            \n",
    "            self.A_log = self.add_weight(\n",
    "                name='A_log',\n",
    "                shape=(self.d_model, self.d_state),\n",
    "                initializer=keras.initializers.Constant(\n",
    "                    np.log(np.abs(A_real) + 1e-8)\n",
    "                ),\n",
    "                trainable=True\n",
    "            )\n",
    "            \n",
    "            self.B = self.add_weight(\n",
    "                name='B',\n",
    "                shape=(self.d_model, self.d_state),\n",
    "                initializer='glorot_uniform',\n",
    "                trainable=True\n",
    "            )\n",
    "            \n",
    "            self.C = self.add_weight(\n",
    "                name='C',\n",
    "                shape=(self.d_model, self.d_state),\n",
    "                initializer='glorot_uniform',\n",
    "                trainable=True\n",
    "            )\n",
    "            \n",
    "            self.D = self.add_weight(\n",
    "                name='D',\n",
    "                shape=(self.d_model,),\n",
    "                initializer='ones',\n",
    "                trainable=True\n",
    "            )\n",
    "            \n",
    "            # Delta (step size) - learned per channel\n",
    "            self.log_delta = self.add_weight(\n",
    "                name='log_delta',\n",
    "                shape=(self.d_model,),\n",
    "                initializer=keras.initializers.Constant(np.log(0.1)),\n",
    "                trainable=True\n",
    "            )\n",
    "            \n",
    "            super().build(input_shape)\n",
    "            \n",
    "        def call(self, inputs):\n",
    "            \"\"\"\n",
    "            Apply SSM to input sequence.\n",
    "            \n",
    "            Args:\n",
    "                inputs: [batch, seq_len, d_model]\n",
    "                \n",
    "            Returns:\n",
    "                outputs: [batch, seq_len, d_model]\n",
    "            \"\"\"\n",
    "            batch_size = tf.shape(inputs)[0]\n",
    "            seq_len = tf.shape(inputs)[1]\n",
    "            \n",
    "            # Discretize\n",
    "            delta = tf.exp(self.log_delta)  # [d_model]\n",
    "            A = -tf.exp(self.A_log)  # [d_model, d_state] - ensure negative\n",
    "            \n",
    "            # Discretized A and B (ZOH approximation)\n",
    "            A_bar = tf.exp(delta[:, None] * A)  # [d_model, d_state]\n",
    "            B_bar = delta[:, None] * self.B  # [d_model, d_state]\n",
    "            \n",
    "            # Recurrent computation (could be parallelized with scan)\n",
    "            def ssm_step(h, x):\n",
    "                # h: [batch, d_model, d_state]\n",
    "                # x: [batch, d_model]\n",
    "                h_new = A_bar * h + B_bar * x[:, :, None]\n",
    "                y = tf.reduce_sum(self.C * h_new, axis=-1) + self.D * x\n",
    "                return h_new, y\n",
    "            \n",
    "            # Initial state\n",
    "            h0 = tf.zeros((batch_size, self.d_model, self.d_state))\n",
    "            \n",
    "            # Transpose for scan: [seq_len, batch, d_model]\n",
    "            inputs_t = tf.transpose(inputs, [1, 0, 2])\n",
    "            \n",
    "            # Run SSM\n",
    "            _, outputs = tf.scan(\n",
    "                lambda h, x: ssm_step(h, x),\n",
    "                inputs_t,\n",
    "                initializer=h0\n",
    "            )\n",
    "            \n",
    "            # Transpose back: [batch, seq_len, d_model]\n",
    "            outputs = tf.transpose(outputs, [1, 0, 2])\n",
    "            \n",
    "            return outputs\n",
    "        \n",
    "        def get_config(self):\n",
    "            config = super().get_config()\n",
    "            config.update({\n",
    "                'd_model': self.d_model,\n",
    "                'd_state': self.d_state,\n",
    "            })\n",
    "            return config\n",
    "    \n",
    "    print(\"S4DKernel layer defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if HAS_TF:\n",
    "    \n",
    "    class SelectiveSSM(layers.Layer):\n",
    "        \"\"\"\n",
    "        Selective State Space Model (Mamba-style).\n",
    "        \n",
    "        Key difference from S4: B, C, delta are functions of the input.\n",
    "        \"\"\"\n",
    "        \n",
    "        def __init__(self, d_model, d_state=16, d_conv=4, expand=2, **kwargs):\n",
    "            super().__init__(**kwargs)\n",
    "            self.d_model = d_model\n",
    "            self.d_state = d_state\n",
    "            self.d_conv = d_conv\n",
    "            self.expand = expand\n",
    "            self.d_inner = d_model * expand\n",
    "            \n",
    "        def build(self, input_shape):\n",
    "            # Input projection\n",
    "            self.in_proj = layers.Dense(self.d_inner * 2, use_bias=False)\n",
    "            \n",
    "            # 1D convolution for local context\n",
    "            self.conv1d = layers.Conv1D(\n",
    "                self.d_inner, \n",
    "                kernel_size=self.d_conv,\n",
    "                padding='causal',\n",
    "                groups=self.d_inner\n",
    "            )\n",
    "            \n",
    "            # Selective projections (input-dependent B, C, delta)\n",
    "            self.x_proj = layers.Dense(self.d_state * 2 + 1, use_bias=False)\n",
    "            \n",
    "            # Fixed A parameter (diagonal, negative for stability)\n",
    "            A = np.arange(1, self.d_state + 1, dtype=np.float32)\n",
    "            self.A_log = self.add_weight(\n",
    "                name='A_log',\n",
    "                shape=(self.d_inner, self.d_state),\n",
    "                initializer=keras.initializers.Constant(\n",
    "                    np.log(np.tile(A, (self.d_inner, 1)))\n",
    "                ),\n",
    "                trainable=True\n",
    "            )\n",
    "            \n",
    "            self.D = self.add_weight(\n",
    "                name='D',\n",
    "                shape=(self.d_inner,),\n",
    "                initializer='ones',\n",
    "                trainable=True\n",
    "            )\n",
    "            \n",
    "            # Output projection\n",
    "            self.out_proj = layers.Dense(self.d_model, use_bias=False)\n",
    "            \n",
    "            super().build(input_shape)\n",
    "            \n",
    "        def call(self, inputs, training=None):\n",
    "            \"\"\"\n",
    "            Args:\n",
    "                inputs: [batch, seq_len, d_model]\n",
    "                \n",
    "            Returns:\n",
    "                outputs: [batch, seq_len, d_model]\n",
    "            \"\"\"\n",
    "            batch_size = tf.shape(inputs)[0]\n",
    "            seq_len = tf.shape(inputs)[1]\n",
    "            \n",
    "            # Project and split\n",
    "            xz = self.in_proj(inputs)  # [batch, seq, d_inner*2]\n",
    "            x, z = tf.split(xz, 2, axis=-1)  # each [batch, seq, d_inner]\n",
    "            \n",
    "            # Conv for local context\n",
    "            x = self.conv1d(x)\n",
    "            x = tf.nn.silu(x)\n",
    "            \n",
    "            # Selective parameters from input\n",
    "            x_proj = self.x_proj(x)  # [batch, seq, d_state*2 + 1]\n",
    "            \n",
    "            # Split into B, C, delta\n",
    "            B = x_proj[:, :, :self.d_state]  # [batch, seq, d_state]\n",
    "            C = x_proj[:, :, self.d_state:2*self.d_state]\n",
    "            delta = tf.nn.softplus(x_proj[:, :, -1:])  # [batch, seq, 1]\n",
    "            \n",
    "            # A is fixed (but learned), negative for stability\n",
    "            A = -tf.exp(self.A_log)  # [d_inner, d_state]\n",
    "            \n",
    "            # Discretize (per timestep due to selective delta)\n",
    "            # A_bar = exp(delta * A), B_bar = delta * B\n",
    "            delta_A = delta[:, :, :, None] * A[None, None, :, :]  # [batch, seq, d_inner, d_state]\n",
    "            A_bar = tf.exp(delta_A)\n",
    "            \n",
    "            delta_B = delta * B  # [batch, seq, d_state]\n",
    "            \n",
    "            # Recurrent SSM\n",
    "            def selective_ssm_step(h, inputs_t):\n",
    "                x_t, A_bar_t, dB_t, C_t = inputs_t\n",
    "                # h: [batch, d_inner, d_state]\n",
    "                # x_t: [batch, d_inner]\n",
    "                # A_bar_t: [batch, d_inner, d_state]\n",
    "                # dB_t: [batch, d_state]\n",
    "                # C_t: [batch, d_state]\n",
    "                \n",
    "                h_new = A_bar_t * h + x_t[:, :, None] * dB_t[:, None, :]\n",
    "                y = tf.reduce_sum(h_new * C_t[:, None, :], axis=-1)\n",
    "                return h_new, y\n",
    "            \n",
    "            # Prepare for scan\n",
    "            h0 = tf.zeros((batch_size, self.d_inner, self.d_state))\n",
    "            \n",
    "            scan_inputs = (\n",
    "                tf.transpose(x, [1, 0, 2]),  # [seq, batch, d_inner]\n",
    "                tf.transpose(A_bar, [1, 0, 2, 3]),  # [seq, batch, d_inner, d_state]\n",
    "                tf.transpose(delta_B, [1, 0, 2]),  # [seq, batch, d_state]\n",
    "                tf.transpose(C, [1, 0, 2]),  # [seq, batch, d_state]\n",
    "            )\n",
    "            \n",
    "            _, y = tf.scan(\n",
    "                selective_ssm_step,\n",
    "                scan_inputs,\n",
    "                initializer=h0\n",
    "            )\n",
    "            \n",
    "            # y: [seq, batch, d_inner]\n",
    "            y = tf.transpose(y, [1, 0, 2])  # [batch, seq, d_inner]\n",
    "            \n",
    "            # Add skip connection (D parameter)\n",
    "            y = y + self.D * x\n",
    "            \n",
    "            # Gate with z\n",
    "            y = y * tf.nn.silu(z)\n",
    "            \n",
    "            # Output projection\n",
    "            return self.out_proj(y)\n",
    "        \n",
    "        def get_config(self):\n",
    "            config = super().get_config()\n",
    "            config.update({\n",
    "                'd_model': self.d_model,\n",
    "                'd_state': self.d_state,\n",
    "                'd_conv': self.d_conv,\n",
    "                'expand': self.expand,\n",
    "            })\n",
    "            return config\n",
    "    \n",
    "    print(\"SelectiveSSM (Mamba-style) layer defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if HAS_TF:\n",
    "    \n",
    "    class MambaBlock(layers.Layer):\n",
    "        \"\"\"\n",
    "        Complete Mamba block with normalization and residual.\n",
    "        \"\"\"\n",
    "        \n",
    "        def __init__(self, d_model, d_state=16, d_conv=4, expand=2, dropout=0.1, **kwargs):\n",
    "            super().__init__(**kwargs)\n",
    "            self.d_model = d_model\n",
    "            self.norm = layers.LayerNormalization(epsilon=1e-6)\n",
    "            self.ssm = SelectiveSSM(d_model, d_state, d_conv, expand)\n",
    "            self.dropout = layers.Dropout(dropout)\n",
    "            \n",
    "        def call(self, inputs, training=None):\n",
    "            x = self.norm(inputs)\n",
    "            x = self.ssm(x, training=training)\n",
    "            x = self.dropout(x, training=training)\n",
    "            return inputs + x\n",
    "        \n",
    "        def get_config(self):\n",
    "            config = super().get_config()\n",
    "            config.update({'d_model': self.d_model})\n",
    "            return config\n",
    "    \n",
    "    print(\"MambaBlock defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Generate Training Data\n",
    "\n",
    "Generate long sequences with degradation patterns - where Mamba excels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_long_degradation_sequences(n_sequences=100, seq_length=1000, n_features=8):\n",
    "    \"\"\"\n",
    "    Generate long degradation sequences for RUL prediction.\n",
    "    \n",
    "    These long sequences (1000+ timesteps) are where Mamba shines\n",
    "    due to its O(n) complexity vs Transformer's O(n²).\n",
    "    \"\"\"\n",
    "    X = []\n",
    "    y = []\n",
    "    \n",
    "    for i in range(n_sequences):\n",
    "        # Random total life (in cycles)\n",
    "        total_life = np.random.randint(200, max(201, seq_length - 50))\n",
    "        \n",
    "        # Time array\n",
    "        t = np.arange(seq_length)\n",
    "        \n",
    "        # RUL at each timestep (capped at 0 after failure)\n",
    "        rul = np.maximum(0, total_life - t)\n",
    "        \n",
    "        # Generate multi-sensor data with degradation\n",
    "        features = np.zeros((seq_length, n_features))\n",
    "        \n",
    "        # Health indicator (decreases over time)\n",
    "        health = 1.0 - (t / total_life).clip(0, 1)\n",
    "        degradation = 1.0 - health\n",
    "        \n",
    "        # Feature 0: Vibration RMS (increases with wear)\n",
    "        base_vib = 0.5 + 2.0 * degradation ** 1.5\n",
    "        features[:, 0] = base_vib + np.random.normal(0, 0.1, seq_length)\n",
    "        \n",
    "        # Feature 1: Temperature (gradual increase)\n",
    "        base_temp = 60 + 40 * degradation\n",
    "        features[:, 1] = base_temp + np.random.normal(0, 2, seq_length)\n",
    "        \n",
    "        # Feature 2: Oil pressure (decreases)\n",
    "        base_pressure = 5.0 - 2.0 * degradation\n",
    "        features[:, 2] = base_pressure + np.random.normal(0, 0.2, seq_length)\n",
    "        \n",
    "        # Feature 3: Motor current (increases with friction)\n",
    "        base_current = 10 + 5 * degradation ** 2\n",
    "        features[:, 3] = base_current + np.random.normal(0, 0.5, seq_length)\n",
    "        \n",
    "        # Feature 4-7: Additional sensors with various patterns\n",
    "        for f in range(4, n_features):\n",
    "            noise_level = 0.1 + 0.2 * degradation\n",
    "            base = np.sin(2 * np.pi * t / (100 + f * 20)) * (1 + degradation)\n",
    "            features[:, f] = base + np.random.normal(0, noise_level, seq_length)\n",
    "        \n",
    "        X.append(features)\n",
    "        y.append(rul)\n",
    "    \n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# Generate data\n",
    "print(\"Generating long degradation sequences...\")\n",
    "X_long, y_long = generate_long_degradation_sequences(n_sequences=200, seq_length=500)\n",
    "print(f\"Generated: X={X_long.shape}, y={y_long.shape}\")\n",
    "print(f\"Sequence length: {X_long.shape[1]} timesteps (Mamba handles this efficiently!)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize example sequences\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 8))\n",
    "\n",
    "for idx, ax_row in enumerate(axes):\n",
    "    seq_idx = idx * 50  # Different sequences\n",
    "    \n",
    "    ax_row[0].plot(X_long[seq_idx, :, 0], label='Vibration', alpha=0.7)\n",
    "    ax_row[0].plot(X_long[seq_idx, :, 1] / 20, label='Temp/20', alpha=0.7)\n",
    "    ax_row[0].set_title(f'Sequence {seq_idx}: Sensor Data')\n",
    "    ax_row[0].legend()\n",
    "    ax_row[0].set_xlabel('Time Steps')\n",
    "    \n",
    "    ax_row[1].plot(y_long[seq_idx], 'r-', linewidth=2)\n",
    "    ax_row[1].set_title(f'Sequence {seq_idx}: RUL')\n",
    "    ax_row[1].set_xlabel('Time Steps')\n",
    "    ax_row[1].set_ylabel('Remaining Useful Life')\n",
    "    ax_row[1].axhline(y=0, color='k', linestyle='--', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{MODEL_DIR}/long_sequence_examples.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(f\"Saved: {MODEL_DIR}/long_sequence_examples.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Build Mamba-based RUL Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if HAS_TF:\n",
    "    \n",
    "    def build_mamba_rul_model(\n",
    "        seq_length,\n",
    "        n_features,\n",
    "        d_model=64,\n",
    "        n_layers=4,\n",
    "        d_state=16,\n",
    "        dropout=0.1\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Build a Mamba-based model for RUL prediction.\n",
    "        \n",
    "        Outputs RUL at each timestep (sequence-to-sequence).\n",
    "        \"\"\"\n",
    "        inputs = keras.Input(shape=(seq_length, n_features))\n",
    "        \n",
    "        # Input projection\n",
    "        x = layers.Dense(d_model)(inputs)\n",
    "        \n",
    "        # Stack Mamba blocks\n",
    "        for i in range(n_layers):\n",
    "            x = MambaBlock(\n",
    "                d_model=d_model,\n",
    "                d_state=d_state,\n",
    "                dropout=dropout,\n",
    "                name=f'mamba_block_{i}'\n",
    "            )(x)\n",
    "        \n",
    "        # Final normalization\n",
    "        x = layers.LayerNormalization(epsilon=1e-6)(x)\n",
    "        \n",
    "        # Output: RUL at each timestep\n",
    "        outputs = layers.Dense(1, activation='relu')(x)\n",
    "        outputs = tf.squeeze(outputs, axis=-1)\n",
    "        \n",
    "        model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "        return model\n",
    "    \n",
    "    # Build model\n",
    "    seq_length = X_long.shape[1]\n",
    "    n_features = X_long.shape[2]\n",
    "    \n",
    "    mamba_rul_model = build_mamba_rul_model(\n",
    "        seq_length=seq_length,\n",
    "        n_features=n_features,\n",
    "        d_model=64,\n",
    "        n_layers=3,  # Fewer layers for faster training\n",
    "        d_state=16\n",
    "    )\n",
    "    \n",
    "    mamba_rul_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if HAS_TF:\n",
    "    # Prepare data\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = np.zeros_like(X_long)\n",
    "    for i in range(X_long.shape[0]):\n",
    "        X_scaled[i] = scaler.fit_transform(X_long[i])\n",
    "    \n",
    "    # Normalize RUL (divide by max possible)\n",
    "    y_normalized = y_long / 500.0  # Max sequence length\n",
    "    \n",
    "    # Train/test split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X_scaled, y_normalized, test_size=0.2, random_state=42\n",
    "    )\n",
    "    \n",
    "    print(f\"Training: {X_train.shape}, Test: {X_test.shape}\")\n",
    "    \n",
    "    # Compile\n",
    "    mamba_rul_model.compile(\n",
    "        optimizer=keras.optimizers.Adam(learning_rate=1e-3),\n",
    "        loss='mse',\n",
    "        metrics=['mae']\n",
    "    )\n",
    "    \n",
    "    # Callbacks\n",
    "    callbacks = [\n",
    "        keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True),\n",
    "        keras.callbacks.ReduceLROnPlateau(factor=0.5, patience=5)\n",
    "    ]\n",
    "    \n",
    "    # Train\n",
    "    print(\"\\nTraining Mamba RUL model...\")\n",
    "    history = mamba_rul_model.fit(\n",
    "        X_train, y_train,\n",
    "        validation_split=0.15,\n",
    "        epochs=50,\n",
    "        batch_size=16,\n",
    "        callbacks=callbacks,\n",
    "        verbose=1\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if HAS_TF:\n",
    "    # Evaluate\n",
    "    y_pred = mamba_rul_model.predict(X_test)\n",
    "    \n",
    "    # Scale back to original RUL values\n",
    "    y_test_orig = y_test * 500\n",
    "    y_pred_orig = y_pred * 500\n",
    "    \n",
    "    # Calculate metrics (on last timestep - when we care most)\n",
    "    rmse = np.sqrt(mean_squared_error(y_test_orig[:, -1], y_pred_orig[:, -1]))\n",
    "    mae = mean_absolute_error(y_test_orig[:, -1], y_pred_orig[:, -1])\n",
    "    r2 = r2_score(y_test_orig[:, -1], y_pred_orig[:, -1])\n",
    "    \n",
    "    print(f\"\\nMamba RUL Model Results (Final Timestep):\")\n",
    "    print(f\"  RMSE: {rmse:.2f} cycles\")\n",
    "    print(f\"  MAE:  {mae:.2f} cycles\")\n",
    "    print(f\"  R²:   {r2:.4f}\")\n",
    "    \n",
    "    # Visualize predictions\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "    \n",
    "    # Training history\n",
    "    axes[0].plot(history.history['loss'], label='Train')\n",
    "    axes[0].plot(history.history['val_loss'], label='Val')\n",
    "    axes[0].set_xlabel('Epoch')\n",
    "    axes[0].set_ylabel('Loss')\n",
    "    axes[0].set_title('Training History')\n",
    "    axes[0].legend()\n",
    "    \n",
    "    # Actual vs Predicted scatter\n",
    "    axes[1].scatter(y_test_orig[:, -1], y_pred_orig[:, -1], alpha=0.5)\n",
    "    axes[1].plot([0, 500], [0, 500], 'r--', label='Perfect')\n",
    "    axes[1].set_xlabel('Actual RUL')\n",
    "    axes[1].set_ylabel('Predicted RUL')\n",
    "    axes[1].set_title(f'Mamba RUL (R²={r2:.3f})')\n",
    "    axes[1].legend()\n",
    "    \n",
    "    # Example sequence prediction\n",
    "    idx = 0\n",
    "    axes[2].plot(y_test_orig[idx], label='Actual', linewidth=2)\n",
    "    axes[2].plot(y_pred_orig[idx], label='Predicted', linewidth=2, alpha=0.7)\n",
    "    axes[2].set_xlabel('Time Step')\n",
    "    axes[2].set_ylabel('RUL')\n",
    "    axes[2].set_title('Sequence RUL Prediction')\n",
    "    axes[2].legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{MODEL_DIR}/mamba_rul_results.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Mamba Anomaly Detection (Autoencoder)\n",
    "\n",
    "Use Mamba as encoder/decoder for reconstruction-based anomaly detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if HAS_TF:\n",
    "    \n",
    "    def build_mamba_autoencoder(\n",
    "        seq_length,\n",
    "        n_features,\n",
    "        d_model=32,\n",
    "        latent_dim=16,\n",
    "        n_layers=2\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Mamba-based Autoencoder for anomaly detection.\n",
    "        \n",
    "        Anomalies have high reconstruction error.\n",
    "        \"\"\"\n",
    "        inputs = keras.Input(shape=(seq_length, n_features))\n",
    "        \n",
    "        # Encoder\n",
    "        x = layers.Dense(d_model)(inputs)\n",
    "        for i in range(n_layers):\n",
    "            x = MambaBlock(d_model, name=f'enc_mamba_{i}')(x)\n",
    "        \n",
    "        # Bottleneck (compress to latent)\n",
    "        x = layers.Dense(latent_dim)(x)\n",
    "        encoded = layers.GlobalAveragePooling1D()(x)\n",
    "        \n",
    "        # Decoder - expand latent to sequence\n",
    "        x = layers.RepeatVector(seq_length)(encoded)\n",
    "        x = layers.Dense(d_model)(x)\n",
    "        for i in range(n_layers):\n",
    "            x = MambaBlock(d_model, name=f'dec_mamba_{i}')(x)\n",
    "        \n",
    "        # Output\n",
    "        outputs = layers.Dense(n_features)(x)\n",
    "        \n",
    "        model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "        return model\n",
    "    \n",
    "    # Build autoencoder\n",
    "    mamba_ae = build_mamba_autoencoder(\n",
    "        seq_length=seq_length,\n",
    "        n_features=n_features,\n",
    "        d_model=32,\n",
    "        latent_dim=8,\n",
    "        n_layers=2\n",
    "    )\n",
    "    \n",
    "    mamba_ae.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if HAS_TF:\n",
    "    # Train on \"normal\" data only (first half of each sequence before degradation)\n",
    "    # This simulates learning what \"healthy\" looks like\n",
    "    X_normal = X_scaled[:, :250, :]  # First 250 timesteps (healthier)\n",
    "    \n",
    "    X_train_ae, X_test_ae = train_test_split(X_normal, test_size=0.2, random_state=42)\n",
    "    \n",
    "    mamba_ae.compile(\n",
    "        optimizer=keras.optimizers.Adam(1e-3),\n",
    "        loss='mse'\n",
    "    )\n",
    "    \n",
    "    print(\"Training Mamba Autoencoder on 'normal' data...\")\n",
    "    ae_history = mamba_ae.fit(\n",
    "        X_train_ae, X_train_ae,\n",
    "        validation_split=0.15,\n",
    "        epochs=30,\n",
    "        batch_size=16,\n",
    "        callbacks=[\n",
    "            keras.callbacks.EarlyStopping(patience=5, restore_best_weights=True)\n",
    "        ],\n",
    "        verbose=1\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if HAS_TF:\n",
    "    # Test anomaly detection\n",
    "    # Normal data should have low reconstruction error\n",
    "    # Anomalous (degraded) data should have high error\n",
    "    \n",
    "    X_normal_test = X_scaled[:, :250, :]  # Normal (healthy)\n",
    "    X_anomaly_test = X_scaled[:, 350:, :]  # Anomalous (degraded)\n",
    "    \n",
    "    # Compute reconstruction errors\n",
    "    recon_normal = mamba_ae.predict(X_normal_test, verbose=0)\n",
    "    recon_anomaly = mamba_ae.predict(X_anomaly_test, verbose=0)\n",
    "    \n",
    "    error_normal = np.mean((X_normal_test - recon_normal) ** 2, axis=(1, 2))\n",
    "    error_anomaly = np.mean((X_anomaly_test - recon_anomaly) ** 2, axis=(1, 2))\n",
    "    \n",
    "    # Visualize\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "    \n",
    "    axes[0].hist(error_normal, bins=30, alpha=0.7, label='Normal', color='green')\n",
    "    axes[0].hist(error_anomaly, bins=30, alpha=0.7, label='Anomaly', color='red')\n",
    "    axes[0].set_xlabel('Reconstruction Error (MSE)')\n",
    "    axes[0].set_ylabel('Count')\n",
    "    axes[0].set_title('Mamba Autoencoder: Anomaly Detection')\n",
    "    axes[0].legend()\n",
    "    \n",
    "    # ROC-like: threshold vs detection\n",
    "    thresholds = np.linspace(0, max(error_anomaly.max(), error_normal.max()), 100)\n",
    "    tpr = [np.mean(error_anomaly > t) for t in thresholds]\n",
    "    fpr = [np.mean(error_normal > t) for t in thresholds]\n",
    "    \n",
    "    axes[1].plot(fpr, tpr, 'b-', linewidth=2)\n",
    "    axes[1].plot([0, 1], [0, 1], 'k--', alpha=0.3)\n",
    "    axes[1].set_xlabel('False Positive Rate')\n",
    "    axes[1].set_ylabel('True Positive Rate')\n",
    "    axes[1].set_title('ROC Curve')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{MODEL_DIR}/mamba_anomaly_detection.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    # Compute AUC\n",
    "    from sklearn.metrics import roc_auc_score\n",
    "    labels = np.concatenate([np.zeros(len(error_normal)), np.ones(len(error_anomaly))])\n",
    "    scores = np.concatenate([error_normal, error_anomaly])\n",
    "    auc = roc_auc_score(labels, scores)\n",
    "    print(f\"\\nMamba Autoencoder AUC: {auc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Mamba Classifier for Fault Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_fault_classification_data(n_samples=500, seq_length=200, n_features=6):\n",
    "    \"\"\"\n",
    "    Generate multi-class fault data for classification.\n",
    "    \"\"\"\n",
    "    X = []\n",
    "    y = []\n",
    "    \n",
    "    fault_types = ['normal', 'unbalance', 'misalignment', 'bearing_fault', 'looseness']\n",
    "    \n",
    "    for _ in range(n_samples):\n",
    "        fault = np.random.choice(fault_types)\n",
    "        \n",
    "        # Base signal (rotating machinery)\n",
    "        t = np.linspace(0, 1, seq_length)\n",
    "        rpm = 1500 + np.random.normal(0, 50)\n",
    "        f_rot = rpm / 60\n",
    "        \n",
    "        features = np.zeros((seq_length, n_features))\n",
    "        \n",
    "        if fault == 'normal':\n",
    "            # Clean signal with small 1x component\n",
    "            features[:, 0] = 0.5 * np.sin(2 * np.pi * f_rot * t)\n",
    "            features[:, 1] = 0.2 * np.sin(4 * np.pi * f_rot * t)\n",
    "            noise_level = 0.1\n",
    "            \n",
    "        elif fault == 'unbalance':\n",
    "            # Strong 1x component\n",
    "            features[:, 0] = 2.0 * np.sin(2 * np.pi * f_rot * t)\n",
    "            features[:, 1] = 0.3 * np.sin(4 * np.pi * f_rot * t)\n",
    "            noise_level = 0.15\n",
    "            \n",
    "        elif fault == 'misalignment':\n",
    "            # Strong 2x component\n",
    "            features[:, 0] = 0.8 * np.sin(2 * np.pi * f_rot * t)\n",
    "            features[:, 1] = 1.5 * np.sin(4 * np.pi * f_rot * t)\n",
    "            features[:, 2] = 0.5 * np.sin(6 * np.pi * f_rot * t)\n",
    "            noise_level = 0.2\n",
    "            \n",
    "        elif fault == 'bearing_fault':\n",
    "            # High frequency components + impulses\n",
    "            features[:, 0] = 0.6 * np.sin(2 * np.pi * f_rot * t)\n",
    "            # BPFO-like frequency\n",
    "            f_bpfo = f_rot * 7.2  # Typical BPFO ratio\n",
    "            features[:, 3] = 0.8 * np.sin(2 * np.pi * f_bpfo * t)\n",
    "            # Random impulses\n",
    "            impulses = np.zeros(seq_length)\n",
    "            impulse_locs = np.random.choice(seq_length, size=10, replace=False)\n",
    "            impulses[impulse_locs] = np.random.uniform(1, 2, 10)\n",
    "            features[:, 4] = impulses\n",
    "            noise_level = 0.25\n",
    "            \n",
    "        elif fault == 'looseness':\n",
    "            # Many harmonics (1x, 2x, 3x, 4x...)\n",
    "            for h in range(1, 6):\n",
    "                features[:, min(h-1, n_features-1)] += (0.5 / h) * np.sin(2 * np.pi * h * f_rot * t)\n",
    "            # Sub-harmonics\n",
    "            features[:, 5] = 0.4 * np.sin(np.pi * f_rot * t)\n",
    "            noise_level = 0.3\n",
    "        \n",
    "        # Add noise\n",
    "        features += np.random.normal(0, noise_level, features.shape)\n",
    "        \n",
    "        X.append(features)\n",
    "        y.append(fault)\n",
    "    \n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# Generate classification data\n",
    "print(\"Generating fault classification data...\")\n",
    "X_clf, y_clf = generate_fault_classification_data(n_samples=1000, seq_length=200)\n",
    "print(f\"Generated: X={X_clf.shape}\")\n",
    "print(f\"Classes: {np.unique(y_clf)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if HAS_TF:\n",
    "    \n",
    "    def build_mamba_classifier(\n",
    "        seq_length,\n",
    "        n_features,\n",
    "        n_classes,\n",
    "        d_model=64,\n",
    "        n_layers=3\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Mamba-based sequence classifier.\n",
    "        \"\"\"\n",
    "        inputs = keras.Input(shape=(seq_length, n_features))\n",
    "        \n",
    "        # Project\n",
    "        x = layers.Dense(d_model)(inputs)\n",
    "        \n",
    "        # Mamba blocks\n",
    "        for i in range(n_layers):\n",
    "            x = MambaBlock(d_model, name=f'clf_mamba_{i}')(x)\n",
    "        \n",
    "        # Global pooling\n",
    "        x = layers.LayerNormalization()(x)\n",
    "        x = layers.GlobalAveragePooling1D()(x)\n",
    "        \n",
    "        # Classification head\n",
    "        x = layers.Dense(32, activation='relu')(x)\n",
    "        x = layers.Dropout(0.3)(x)\n",
    "        outputs = layers.Dense(n_classes, activation='softmax')(x)\n",
    "        \n",
    "        model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "        return model\n",
    "    \n",
    "    # Encode labels\n",
    "    le = LabelEncoder()\n",
    "    y_encoded = le.fit_transform(y_clf)\n",
    "    n_classes = len(le.classes_)\n",
    "    \n",
    "    # Scale features\n",
    "    X_clf_scaled = np.zeros_like(X_clf)\n",
    "    for i in range(X_clf.shape[0]):\n",
    "        X_clf_scaled[i] = StandardScaler().fit_transform(X_clf[i])\n",
    "    \n",
    "    # Split\n",
    "    X_train_clf, X_test_clf, y_train_clf, y_test_clf = train_test_split(\n",
    "        X_clf_scaled, y_encoded, test_size=0.2, random_state=42, stratify=y_encoded\n",
    "    )\n",
    "    \n",
    "    # Build model\n",
    "    mamba_clf = build_mamba_classifier(\n",
    "        seq_length=X_clf.shape[1],\n",
    "        n_features=X_clf.shape[2],\n",
    "        n_classes=n_classes,\n",
    "        d_model=48,\n",
    "        n_layers=2\n",
    "    )\n",
    "    \n",
    "    mamba_clf.compile(\n",
    "        optimizer=keras.optimizers.Adam(1e-3),\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    mamba_clf.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if HAS_TF:\n",
    "    print(\"Training Mamba Classifier...\")\n",
    "    clf_history = mamba_clf.fit(\n",
    "        X_train_clf, y_train_clf,\n",
    "        validation_split=0.15,\n",
    "        epochs=40,\n",
    "        batch_size=32,\n",
    "        callbacks=[\n",
    "            keras.callbacks.EarlyStopping(patience=8, restore_best_weights=True),\n",
    "            keras.callbacks.ReduceLROnPlateau(factor=0.5, patience=4)\n",
    "        ],\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    # Evaluate\n",
    "    y_pred_clf = mamba_clf.predict(X_test_clf).argmax(axis=1)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"Mamba Classifier Results:\")\n",
    "    print(\"=\"*50)\n",
    "    print(classification_report(y_test_clf, y_pred_clf, target_names=le.classes_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if HAS_TF:\n",
    "    # Confusion matrix\n",
    "    cm = confusion_matrix(y_test_clf, y_pred_clf)\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # Training curves\n",
    "    axes[0].plot(clf_history.history['accuracy'], label='Train')\n",
    "    axes[0].plot(clf_history.history['val_accuracy'], label='Val')\n",
    "    axes[0].set_xlabel('Epoch')\n",
    "    axes[0].set_ylabel('Accuracy')\n",
    "    axes[0].set_title('Mamba Classifier Training')\n",
    "    axes[0].legend()\n",
    "    \n",
    "    # Confusion matrix\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "                xticklabels=le.classes_, yticklabels=le.classes_, ax=axes[1])\n",
    "    axes[1].set_xlabel('Predicted')\n",
    "    axes[1].set_ylabel('Actual')\n",
    "    axes[1].set_title('Confusion Matrix')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{MODEL_DIR}/mamba_classifier_results.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Comparison: Mamba vs Transformer Efficiency\n",
    "\n",
    "Demonstrate the computational advantage of Mamba for long sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if HAS_TF:\n",
    "    import time\n",
    "    \n",
    "    # Compare inference time for different sequence lengths\n",
    "    seq_lengths = [100, 250, 500, 750, 1000]\n",
    "    mamba_times = []\n",
    "    n_features_test = 8\n",
    "    batch_size_test = 16\n",
    "    \n",
    "    print(\"Measuring Mamba inference time for different sequence lengths...\")\n",
    "    print(f\"Batch size: {batch_size_test}\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    for seq_len in seq_lengths:\n",
    "        # Build fresh model\n",
    "        model = build_mamba_rul_model(\n",
    "            seq_length=seq_len,\n",
    "            n_features=n_features_test,\n",
    "            d_model=64,\n",
    "            n_layers=3\n",
    "        )\n",
    "        \n",
    "        # Random input\n",
    "        X_test_time = np.random.randn(batch_size_test, seq_len, n_features_test).astype(np.float32)\n",
    "        \n",
    "        # Warmup\n",
    "        _ = model.predict(X_test_time, verbose=0)\n",
    "        \n",
    "        # Measure\n",
    "        start = time.time()\n",
    "        for _ in range(10):\n",
    "            _ = model.predict(X_test_time, verbose=0)\n",
    "        elapsed = (time.time() - start) / 10\n",
    "        \n",
    "        mamba_times.append(elapsed)\n",
    "        print(f\"  Seq Length {seq_len:4d}: {elapsed*1000:.2f} ms\")\n",
    "    \n",
    "    # Plot scaling\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.plot(seq_lengths, mamba_times, 'bo-', linewidth=2, markersize=8, label='Mamba (O(n))')\n",
    "    \n",
    "    # Theoretical O(n²) for comparison\n",
    "    t_base = mamba_times[0]\n",
    "    theoretical_n2 = [t_base * (s / seq_lengths[0])**2 for s in seq_lengths]\n",
    "    plt.plot(seq_lengths, theoretical_n2, 'r--', linewidth=2, label='Theoretical O(n²)')\n",
    "    \n",
    "    plt.xlabel('Sequence Length')\n",
    "    plt.ylabel('Inference Time (seconds)')\n",
    "    plt.title('Mamba Scaling: Linear Complexity')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.savefig(f'{MODEL_DIR}/mamba_scaling.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Save Models and Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if HAS_TF:\n",
    "    # Save Keras models\n",
    "    mamba_rul_model.save(f'{MODEL_DIR}/mamba_rul_model.keras')\n",
    "    mamba_ae.save(f'{MODEL_DIR}/mamba_autoencoder.keras')\n",
    "    mamba_clf.save(f'{MODEL_DIR}/mamba_classifier.keras')\n",
    "    \n",
    "    # Save metadata\n",
    "    metadata = {\n",
    "        'model_type': 'Mamba (Selective State Space Model)',\n",
    "        'models': {\n",
    "            'rul': {\n",
    "                'file': 'mamba_rul_model.keras',\n",
    "                'seq_length': int(seq_length),\n",
    "                'n_features': int(n_features),\n",
    "                'metrics': {'rmse': float(rmse), 'mae': float(mae), 'r2': float(r2)}\n",
    "            },\n",
    "            'autoencoder': {\n",
    "                'file': 'mamba_autoencoder.keras',\n",
    "                'seq_length': 250,\n",
    "                'auc': float(auc)\n",
    "            },\n",
    "            'classifier': {\n",
    "                'file': 'mamba_classifier.keras',\n",
    "                'classes': le.classes_.tolist()\n",
    "            }\n",
    "        },\n",
    "        'advantages': [\n",
    "            'Linear O(n) complexity for long sequences',\n",
    "            'Input-selective parameters',\n",
    "            'Memory efficient inference',\n",
    "            'Captures long-range dependencies'\n",
    "        ],\n",
    "        'recommended_use_cases': [\n",
    "            'Long monitoring windows (1000+ timesteps)',\n",
    "            'Edge deployment with memory constraints',\n",
    "            'Real-time streaming data'\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    with open(f'{MODEL_DIR}/mamba_metadata.json', 'w') as f:\n",
    "        json.dump(metadata, f, indent=2)\n",
    "    \n",
    "    print(f\"\\nModels saved to {MODEL_DIR}/\")\n",
    "    print(\"Files:\")\n",
    "    for f in os.listdir(MODEL_DIR):\n",
    "        print(f\"  - {f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Node-RED Integration Example\n",
    "\n",
    "JavaScript code for integrating Mamba predictions in Node-RED."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_red_code = '''\n",
    "// Node-RED Function Node: Mamba RUL Prediction\n",
    "// Requires: tensorflowjs converted model\n",
    "\n",
    "// Buffer for collecting sequence data\n",
    "const SEQ_LENGTH = 500;  // Mamba handles long sequences efficiently\n",
    "const N_FEATURES = 8;\n",
    "\n",
    "// Initialize buffer\n",
    "if (!context.buffer) {\n",
    "    context.buffer = [];\n",
    "}\n",
    "\n",
    "// Add new reading\n",
    "context.buffer.push([\n",
    "    msg.payload.vibration,\n",
    "    msg.payload.temperature,\n",
    "    msg.payload.pressure,\n",
    "    msg.payload.current,\n",
    "    msg.payload.sensor5,\n",
    "    msg.payload.sensor6,\n",
    "    msg.payload.sensor7,\n",
    "    msg.payload.sensor8\n",
    "]);\n",
    "\n",
    "// Keep only last SEQ_LENGTH readings\n",
    "if (context.buffer.length > SEQ_LENGTH) {\n",
    "    context.buffer.shift();\n",
    "}\n",
    "\n",
    "// Need minimum data before prediction\n",
    "if (context.buffer.length < 100) {\n",
    "    msg.payload = {\n",
    "        status: \"collecting\",\n",
    "        samples: context.buffer.length,\n",
    "        required: 100\n",
    "    };\n",
    "    return msg;\n",
    "}\n",
    "\n",
    "// Pad if needed\n",
    "let sequence = context.buffer.slice();\n",
    "while (sequence.length < SEQ_LENGTH) {\n",
    "    sequence.unshift(sequence[0]);  // Pad with first reading\n",
    "}\n",
    "\n",
    "// Normalize (using stored scaler params)\n",
    "const means = flow.get(\"scaler_means\") || new Array(N_FEATURES).fill(0);\n",
    "const stds = flow.get(\"scaler_stds\") || new Array(N_FEATURES).fill(1);\n",
    "\n",
    "let normalized = sequence.map(row => \n",
    "    row.map((val, i) => (val - means[i]) / stds[i])\n",
    ");\n",
    "\n",
    "// Prepare for model\n",
    "msg.payload = {\n",
    "    input: [normalized],  // Batch of 1\n",
    "    model: \"mamba_rul\",\n",
    "    description: \"Mamba SSM - efficient for long sequences\"\n",
    "};\n",
    "\n",
    "return msg;\n",
    "''';\n",
    "\n",
    "print(\"Node-RED Integration Code:\")\n",
    "print(\"=\" * 50)\n",
    "print(node_red_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrated **Mamba (Selective State Space Models)** for Predictive Maintenance:\n",
    "\n",
    "### Key Advantages of Mamba for PdM:\n",
    "\n",
    "| Feature | Benefit |\n",
    "|---------|--------|\n",
    "| **O(n) Complexity** | Handles sequences 10x longer than Transformers |\n",
    "| **Selective Mechanism** | Adapts to different sensor inputs dynamically |\n",
    "| **Memory Efficient** | Constant memory during inference |\n",
    "| **Long-Range Dependencies** | Captures slow degradation trends |\n",
    "\n",
    "### Models Created:\n",
    "\n",
    "1. **Mamba RUL Model** - Sequence-to-sequence RUL prediction\n",
    "2. **Mamba Autoencoder** - Reconstruction-based anomaly detection\n",
    "3. **Mamba Classifier** - Fault type classification\n",
    "\n",
    "### When to Use Mamba over Transformers:\n",
    "\n",
    "- Sequence length > 500 timesteps\n",
    "- Memory-constrained edge devices\n",
    "- Real-time streaming applications\n",
    "- Monitoring slow degradation processes"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
